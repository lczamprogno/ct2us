{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lczamprogno/ct2us/blob/main/CT2US.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzmsGLmOGUvw"
      },
      "source": [
        "# CT2US\n",
        "\n",
        "This tool is intended to automate the generation of simulated ultrasound image and label pairs from ct volumes (.nii/.nii.gz).\n",
        "\n",
        "---\n",
        "\n",
        "### Purpose\n",
        "Intended to be capable of supplementing datasets for ultrasound image labeling.\n",
        "\n",
        "## Expandability\n",
        "Image generation process is very dependant on tissue attenuation, so specialized US renderers would be necessary/ideal to expand this tool to work on other body parts. For this purpose, much of the following code has hence been designed with modularity as a core goal, so that new methods can be added/replaced, as for example the segmentation quality or speed could have a significant impact on overall results. \n",
        "\n",
        "---\n",
        "\n",
        "## Current use:\n",
        "- ![example](../assets/Full%20Demo.gif)\n",
        "  \n",
        "\n",
        "## Further goals:\n",
        "- code for two alternate optimized segmentation pipelines is still being developed\n",
        "  - one focusing on avoiding internal totalsegmentator steps being saved to memory [ ]\n",
        "  - another further optimizes by properly using gpu and cpu acceleration. [ ]\n",
        "\n",
        "- Throughout the process of acquiring the necessary information for the rendering - and even for visualization - could be a useful resource. With that in mind, enabling saving these intermediary results is a goal, albeit one that has yet to be fully achieved. The download tab is intended to serve the purpose of adjusting saved information. [ ]\n",
        "\n",
        "- Improved version of the totalsegmentator nnunet is still WIP. Once that is taken care of, pluging this in the pipeline with the stacked assemble should yield a significant speed up. [ ]\n",
        "\n",
        "- Currently there is some support for cpu, but this script is mainly designed with gpu acceleration in mind. For cpu optimization, the label_map dict needs to be used to gather all the label names which are contained. This list then can be used by just plugging it in the totalsegmentator function as a parameter, and the built in ROI should yield faster results. [ ]\n",
        "\n",
        "- There seemed to be no significant improvement through using the ROI feature in the gpu version. Here, it is important to note that allocation of gpu memory and copying data over has a significant memory cost, which is actually one of the main improvements of the new totalsegmentator pipeline implemented. [ ]\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This needs to be run once and then the session needs to be restarted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "8-C6Ik3Z48SC",
        "outputId": "fd217900-a75a-4d6c-cdd6-43a103b14a1c"
      },
      "outputs": [],
      "source": [
        "%pip install colab\n",
        "%pip install totalsegmentator numba cupy-cuda12x torchvision xmltodict torchio cucim \"bokeh>=3.1.0\" di gradio pathlib trimesh[easy]\n",
        "\n",
        "#numpy-stl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y52AvG-caZ6u"
      },
      "source": [
        "# Classes and methods are here\n",
        "\n",
        "## Run this block\n",
        "\n",
        "IMPORTANT: Acquire a totalsegmentator key (https://backend.totalsegmentator.com/license-academic/) and set google colab secret as shown:\n",
        "\n",
        "![a](../assets/secret.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "9HH9ibpeNCYr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No module named 'google'\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "from pathlib import PosixPath as pthlib\n",
        "from zipfile import ZipFile\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from math import pi\n",
        "import tqdm\n",
        "\n",
        "from itertools import islice\n",
        "import glob\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "from numpy import uint8\n",
        "\n",
        "\n",
        "from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO\n",
        "from nibabel import nifti1\n",
        "\n",
        "import totalsegmentator.python_api as ts\n",
        "from totalsegmentator.config import setup_nnunet, setup_totalseg, set_config_key, get_weights_dir\n",
        "\n",
        "this_folder = pthlib(\"../CT2US\").resolve()\n",
        "\n",
        "sys.path.append(this_folder)\n",
        "ts_cfg_path = pthlib.joinpath(this_folder, \".totalsegmentator\")\n",
        "ts_cfg_path.mkdir(exist_ok=True, parents=True)\n",
        "os.environ[\"TOTALSEG_HOME_DIR\"] = str(ts_cfg_path)\n",
        "\n",
        "try: \n",
        "    from google.colab import userdata\n",
        "    license = userdata.get('license_key')\n",
        "except ImportError as e:\n",
        "    print(e)\n",
        "\n",
        "from totalsegmentator.libs import download_model_with_license_and_unpack, download_url_and_unpack\n",
        "from totalsegmentator import resampling as rs\n",
        "from totalsegmentator.map_to_binary import commercial_models\n",
        "\n",
        "setup_nnunet()\n",
        "setup_totalseg()\n",
        "\n",
        "ts.set_license_number(license)\n",
        "\n",
        "from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
        "from nnunetv2.inference.sliding_window_prediction import compute_gaussian\n",
        "\n",
        "from cucim.skimage.transform import resize\n",
        "from batchgenerators.utilities.file_and_folder_operations import join\n",
        "\n",
        "from numba import jit, njit, cuda\n",
        "\n",
        "try:\n",
        "    import cupy as cp\n",
        "    import cupyx.scipy.ndimage as cusci\n",
        "except ImportError:\n",
        "    print(\"Error loading cupy and cusci, GPU not available?\")\n",
        "\n",
        "import scipy.ndimage\n",
        "import scipy\n",
        "\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from torch import device\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import gradio as gr\n",
        "import trimesh as tri\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda\", 0) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "torch.set_default_device(device)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "US slice simulation code(from https://github.com/danivelikova/lotus/blob/main/models/us_rendering_model.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRLJBwammqoh",
        "outputId": "b8b193b4-71d7-4fe6-cd67-f818b6bb160d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/enter/envs/ct2us/lib/python3.10/site-packages/torch/utils/_device.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# 2 - lung; 3 - fat; 4 - vessel; 6 - kidney; 8 - muscle; 9 - background; 11 - liver; 12 - soft tissue; 13 - bone;\n",
        "# Default Parameters from: https://github.com/Blito/burgercpp/blob/master/examples/ircad11/liver.scene , labels 8, 9 and 12 approximated from other labels\n",
        "\n",
        "                     # indexes:           2       3     4     6     8      9     11    12    13\n",
        "acoustic_imped_def_dict = torch.tensor([0.0004, 1.38, 1.61,  1.62, 1.62,  0.3,  1.65, 1.63, 7.8], requires_grad=True).to(device=device)    # Z in MRayl\n",
        "attenuation_def_dict =    torch.tensor([1.64,   0.63, 0.18,  1.0,  1.09, 0.54,  0.7,  0.54, 5.0], requires_grad=True).to(device=device)    # alpha in dB cm^-1 at 1 MHz\n",
        "mu_0_def_dict =           torch.tensor([0.78,   0.5,  0.001, 0.45,  0.45,  0.3,  0.4, 0.45, 0.78], requires_grad=True).to(device=device) # mu_0 - scattering_mu   mean brightness\n",
        "mu_1_def_dict =           torch.tensor([0.56,   0.5,  0.0,   0.6,  0.64,  0.2,  0.8,  0.64, 0.56], requires_grad=True).to(device=device) # mu_1 - scattering density, Nr of scatterers/voxel\n",
        "sigma_0_def_dict =        torch.tensor([0.1,    0.0,  0.01,  0.3,  0.1,   0.0,  0.14, 0.1,  0.1], requires_grad=True).to(device=device) # sigma_0 - scattering_sigma - brightness std\n",
        "\n",
        "\n",
        "alpha_coeff_boundary_map = 0.1\n",
        "beta_coeff_scattering = 10  #100 approximates it closer\n",
        "TGC = 8\n",
        "CLAMP_VALS = True\n",
        "\n",
        "\n",
        "def gaussian_kernel(size: int, mean: float, std: float):\n",
        "    d1 = torch.distributions.Normal(mean, std)\n",
        "    d2 = torch.distributions.Normal(mean, std*3)\n",
        "    vals_x = d1.log_prob(torch.arange(-size, size+1, dtype=torch.float32)).exp()\n",
        "    vals_y = d2.log_prob(torch.arange(-size, size+1, dtype=torch.float32)).exp()\n",
        "\n",
        "    gauss_kernel = torch.einsum('i,j->ij', vals_x, vals_y)\n",
        "\n",
        "    return gauss_kernel / torch.sum(gauss_kernel).reshape(1, 1)\n",
        "\n",
        "g_kernel = gaussian_kernel(3, 0., 0.5)\n",
        "g_kernel = torch.tensor(g_kernel[None, None, :, :], dtype=torch.float32).to(device=device)\n",
        "\n",
        "\n",
        "class UltrasoundRendering(torch.nn.Module):\n",
        "    def __init__(self, params, default_param=False):\n",
        "        super(UltrasoundRendering, self).__init__()\n",
        "        self.params = params\n",
        "\n",
        "        if default_param:\n",
        "            self.acoustic_impedance_dict = acoustic_imped_def_dict.detach().clone()\n",
        "            self.attenuation_dict = attenuation_def_dict.detach().clone()\n",
        "            self.mu_0_dict = mu_0_def_dict.detach().clone()\n",
        "            self.mu_1_dict = mu_1_def_dict.detach().clone()\n",
        "            self.sigma_0_dict = sigma_0_def_dict.detach().clone()\n",
        "\n",
        "        else:\n",
        "            self.acoustic_impedance_dict = torch.nn.Parameter(acoustic_imped_def_dict)\n",
        "            self.attenuation_dict = torch.nn.Parameter(attenuation_def_dict)\n",
        "\n",
        "            self.mu_0_dict = torch.nn.Parameter(mu_0_def_dict)\n",
        "            self.mu_1_dict = torch.nn.Parameter(mu_1_def_dict)\n",
        "            self.sigma_0_dict = torch.nn.Parameter(sigma_0_def_dict)\n",
        "\n",
        "        self.labels = [\"lung\", \"fat\", \"vessel\", \"kidney\", \"muscle\", \"background\", \"liver\", \"soft tissue\", \"bone\"]\n",
        "\n",
        "        self.attenuation_medium_map, self.acoustic_imped_map, self.sigma_0_map, self.mu_1_map, self.mu_0_map  = ([] for i in range(5))\n",
        "\n",
        "\n",
        "    def map_dict_to_array(self, dictionary, arr):\n",
        "        mapping_keys = torch.tensor([2, 3, 4, 6, 8, 9, 11, 12, 13], dtype=torch.long).to(device=device)\n",
        "        keys = torch.unique(arr).to(device=device)\n",
        "\n",
        "        index = torch.where(mapping_keys[None, :] == keys[:, None])[1]\n",
        "        values = torch.gather(dictionary, dim=0, index=index)\n",
        "        values = values.to(device=device)\n",
        "        # values.register_hook(lambda grad: print(grad))    # check the gradient during training\n",
        "\n",
        "        mapping = torch.zeros(keys.max().item() + 1).to(device=device)\n",
        "        mapping[keys] = values\n",
        "        return mapping[arr]\n",
        "\n",
        "\n",
        "    def plot_fig(self, fig, fig_name, grayscale):\n",
        "        save_dir='results_test/'\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "\n",
        "        plt.clf()\n",
        "\n",
        "        if torch.is_tensor(fig):\n",
        "            fig = fig.cpu().detach().numpy()\n",
        "\n",
        "        if grayscale:\n",
        "            plt.imshow(fig, cmap='gray', vmin=0, vmax=1, interpolation='none', norm=None)\n",
        "        else:\n",
        "            plt.imshow(fig, interpolation='none', norm=None)\n",
        "        plt.axis('off')\n",
        "        plt.savefig(save_dir + fig_name + '.png', bbox_inches='tight',transparent=True, pad_inches=0)\n",
        "\n",
        "\n",
        "    def clamp_map_ranges(self):\n",
        "        self.attenuation_medium_map = torch.clamp(self.attenuation_medium_map, 0, 10)\n",
        "        self.acoustic_imped_map = torch.clamp(self.acoustic_imped_map, 0, 10)\n",
        "        self.sigma_0_map = torch.clamp(self.sigma_0_map, 0, 1)\n",
        "        self.mu_1_map = torch.clamp(self.mu_1_map, 0, 1)\n",
        "        self.mu_0_map = torch.clamp(self.mu_0_map, 0, 1)\n",
        "\n",
        "\n",
        "    def rendering(self, H, W, z_vals=None, refl_map=None, boundary_map=None):\n",
        "\n",
        "        dists = torch.abs(z_vals[..., :-1, None] - z_vals[..., 1:, None])     # dists.shape=(W, H-1, 1)\n",
        "        dists = dists.squeeze(-1)                                             # dists.shape=(W, H-1)\n",
        "        dists = torch.cat([dists, dists[:, -1, None]], dim=-1)                # dists.shape=(W, H)\n",
        "\n",
        "        attenuation = torch.exp(-self.attenuation_medium_map * dists)\n",
        "        attenuation_total = torch.cumprod(attenuation, dim=1, dtype=torch.float32, out=None)\n",
        "\n",
        "        gain_coeffs = np.linspace(1, TGC, attenuation_total.shape[1])\n",
        "        gain_coeffs = np.tile(gain_coeffs, (attenuation_total.shape[0], 1))\n",
        "        gain_coeffs = torch.tensor(gain_coeffs).to(device=device)\n",
        "        attenuation_total = attenuation_total * gain_coeffs     # apply TGC\n",
        "\n",
        "        reflection_total = torch.cumprod(1. - refl_map * boundary_map, dim=1, dtype=torch.float32, out=None)\n",
        "        reflection_total = reflection_total.squeeze(-1)\n",
        "        reflection_total_plot = torch.log(reflection_total + torch.finfo(torch.float32).eps)\n",
        "\n",
        "        texture_noise = torch.randn(H, W, dtype=torch.float32).to(device=device)\n",
        "        scattering_probability = torch.randn(H, W, dtype=torch.float32).to(device=device)\n",
        "\n",
        "        scattering_zero = torch.zeros(H, W, dtype=torch.float32).to(device=device)\n",
        "\n",
        "        z = self.mu_1_map - scattering_probability\n",
        "        sigmoid_map = torch.sigmoid(beta_coeff_scattering * z)\n",
        "\n",
        "        # approximating  Eq. (4) to be differentiable:\n",
        "        # where(scattering_probability <= mu_1_map,\n",
        "        #                     texture_noise * sigma_0_map + mu_0_map,\n",
        "        #                     scattering_zero)\n",
        "        scatterers_map =  (sigmoid_map) * (texture_noise * self.sigma_0_map + self.mu_0_map) + (1 -sigmoid_map) * scattering_zero   # Eq. (6)\n",
        "\n",
        "        psf_scatter_conv = torch.nn.functional.conv2d(input=scatterers_map[None, None, :, :], weight=g_kernel, stride=1, padding=\"same\")\n",
        "        psf_scatter_conv = psf_scatter_conv.squeeze()\n",
        "\n",
        "        b = attenuation_total * psf_scatter_conv    # Eq. (3)\n",
        "\n",
        "        border_convolution = torch.nn.functional.conv2d(input=boundary_map[None, None, :, :], weight=g_kernel, stride=1, padding=\"same\")\n",
        "        border_convolution = border_convolution.squeeze()\n",
        "\n",
        "        r = attenuation_total * reflection_total * refl_map * border_convolution # Eq. (2)\n",
        "\n",
        "        intensity_map = b + r   # Eq. (1)\n",
        "        intensity_map = intensity_map.squeeze()\n",
        "        intensity_map = torch.clamp(intensity_map, 0, 1)\n",
        "\n",
        "        return intensity_map, attenuation_total, reflection_total_plot, scatterers_map, scattering_probability, border_convolution, texture_noise, b, r\n",
        "\n",
        "\n",
        "    def render_rays(self, W, H):\n",
        "        N_rays = W\n",
        "        t_vals = torch.linspace(0., 1., H).to(device=device)   # 0-1 linearly spaced, shape H\n",
        "        z_vals = t_vals.unsqueeze(0).expand(N_rays , -1) * 4\n",
        "\n",
        "        return z_vals\n",
        "\n",
        "    # warp the linear US image to approximate US image from curvilinear US probe\n",
        "    def warp_img(self, inputImage):\n",
        "        resultWidth = 360\n",
        "        resultHeight = 220\n",
        "        centerX = resultWidth / 2\n",
        "        centerY = -120.0\n",
        "        maxAngle =  60.0 / 2 / 180 * pi #rad\n",
        "        minAngle = -maxAngle\n",
        "        minRadius = 140.0\n",
        "        maxRadius = 340.0\n",
        "\n",
        "        h, w = inputImage.squeeze().shape\n",
        "\n",
        "        import torch.nn.functional as F\n",
        "\n",
        "        # Create x and y grids\n",
        "        x = torch.arange(resultWidth).float() - centerX\n",
        "        y = torch.arange(resultHeight).float() - centerY\n",
        "        xx, yy = torch.meshgrid(x, y)\n",
        "\n",
        "        # Calculate angle and radius\n",
        "        angle = torch.atan2(xx, yy)\n",
        "        radius = torch.sqrt(xx ** 2 + yy ** 2)\n",
        "\n",
        "        # Create masks for angle and radius\n",
        "        angle_mask = (angle > minAngle) & (angle < maxAngle)\n",
        "        radius_mask = (radius > minRadius) & (radius < maxRadius)\n",
        "\n",
        "        # Calculate original column and row\n",
        "        origCol = (angle - minAngle) / (maxAngle - minAngle) * w\n",
        "        origRow = (radius - minRadius) / (maxRadius - minRadius) * h\n",
        "\n",
        "        # Reshape input image to be a batch of 1 image\n",
        "        inputImage = inputImage.float().unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        # Scale original column and row to be in the range [-1, 1]\n",
        "        origCol = origCol / (w - 1) * 2 - 1\n",
        "        origRow = origRow / (h - 1) * 2 - 1\n",
        "\n",
        "        # Transpose input image to have channels first\n",
        "        inputImage = inputImage.permute(0, 1, 3, 2)\n",
        "\n",
        "        # Use grid_sample to interpolate\n",
        "        grid = torch.stack([origCol, origRow], dim=-1).unsqueeze(0).to(device)\n",
        "        resultImage = F.grid_sample(inputImage, grid, mode='bilinear', align_corners=True)\n",
        "\n",
        "        # Apply masks and set values outside of mask to 0\n",
        "        resultImage[~(angle_mask.unsqueeze(0).unsqueeze(0) & radius_mask.unsqueeze(0).unsqueeze(0))] = 0.0\n",
        "        resultImage_resized = transforms.Resize((256,256))(resultImage).float().squeeze()\n",
        "\n",
        "        return resultImage_resized\n",
        "\n",
        "\n",
        "    def forward(self, ct_slice):\n",
        "        if self.params[\"debug\"]: self.plot_fig(ct_slice, \"ct_slice\", False)\n",
        "\n",
        "        #init tissue maps\n",
        "        #generate 2D acousttic_imped map\n",
        "        self.acoustic_imped_map = self.map_dict_to_array(self.acoustic_impedance_dict, ct_slice)#.astype('int64'))\n",
        "\n",
        "        #generate 2D attenuation map\n",
        "        self.attenuation_medium_map = self.map_dict_to_array(self.attenuation_dict, ct_slice)\n",
        "\n",
        "        if self.params[\"debug\"]:\n",
        "            self.plot_fig(self.acoustic_imped_map, \"acoustic_imped_map\", False)\n",
        "            self.plot_fig(self.attenuation_medium_map, \"attenuation_medium_map\", False)\n",
        "\n",
        "        self.mu_0_map = self.map_dict_to_array(self.mu_0_dict, ct_slice)\n",
        "\n",
        "        self.mu_1_map = self.map_dict_to_array(self.mu_1_dict, ct_slice)\n",
        "\n",
        "        self.sigma_0_map = self.map_dict_to_array(self.sigma_0_dict, ct_slice)\n",
        "\n",
        "        self.acoustic_imped_map = torch.rot90(self.acoustic_imped_map, 1, [0, 1])\n",
        "        diff_arr = torch.diff(self.acoustic_imped_map, dim=0)\n",
        "\n",
        "        diff_arr = torch.cat((torch.zeros(diff_arr.shape[1], dtype=torch.float32).unsqueeze(0).to(device=device), diff_arr))\n",
        "\n",
        "        boundary_map =  -torch.exp(-(diff_arr**2)/alpha_coeff_boundary_map) + 1\n",
        "\n",
        "        boundary_map = torch.rot90(boundary_map, 3, [0, 1])\n",
        "\n",
        "        if self.params[\"debug\"]:\n",
        "           self.plot_fig(diff_arr, \"diff_arr\", False)\n",
        "           self.plot_fig(boundary_map, \"boundary_map\", True)\n",
        "\n",
        "        shifted_arr = torch.roll(self.acoustic_imped_map, -1, dims=0)\n",
        "        shifted_arr[-1:] = 0\n",
        "\n",
        "        sum_arr = self.acoustic_imped_map + shifted_arr\n",
        "        sum_arr[sum_arr == 0] = 1\n",
        "        div = diff_arr / sum_arr\n",
        "\n",
        "        refl_map = div ** 2\n",
        "        refl_map = torch.sigmoid(refl_map)      # 1 / (1 + (-refl_map).exp())\n",
        "        refl_map = torch.rot90(refl_map, 3, [0, 1])\n",
        "\n",
        "        if self.params[\"debug\"]: self.plot_fig(refl_map, \"refl_map\", True)\n",
        "\n",
        "        z_vals = self.render_rays(ct_slice.shape[0], ct_slice.shape[1])\n",
        "\n",
        "        if CLAMP_VALS:\n",
        "            self.clamp_map_ranges()\n",
        "\n",
        "        ret_list = self.rendering(ct_slice.shape[0], ct_slice.shape[1], z_vals=z_vals, refl_map=refl_map, boundary_map=boundary_map)\n",
        "\n",
        "        intensity_map  = ret_list[0]\n",
        "\n",
        "        if self.params[\"debug\"]:\n",
        "            self.plot_fig(intensity_map, \"intensity_map\", True)\n",
        "\n",
        "            result_list = [\"intensity_map\", \"attenuation_total\", \"reflection_total\",\n",
        "                            \"scatters_map\", \"scattering_probability\", \"border_convolution\",\n",
        "                            \"texture_noise\", \"b\", \"r\"]\n",
        "\n",
        "            for k in range(len(ret_list)):\n",
        "                result_np = ret_list[k]\n",
        "                if torch.is_tensor(result_np):\n",
        "                    result_np = result_np.detach().cpu().numpy()\n",
        "\n",
        "                if k==2:\n",
        "                    self.plot_fig(result_np, result_list[k], False)\n",
        "                else:\n",
        "                    self.plot_fig(result_np, result_list[k], True)\n",
        "                # print(result_list[k], \", \", result_np.shape)\n",
        "\n",
        "        intensity_map_masked = self.warp_img(intensity_map)\n",
        "        intensity_map_masked = torch.rot90(intensity_map_masked, 3)\n",
        "\n",
        "        if self.params[\"debug\"]:  self.plot_fig(intensity_map_masked, \"intensity_map_masked\", True)\n",
        "\n",
        "        return intensity_map_masked\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Segmentation, Composition and US slicing code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "FX05Q45ymtFo"
      },
      "outputs": [],
      "source": [
        "total_lmap = {\"0\": 0, \"1\": 12, \"2\": 6, \"3\": 6, \"4\": 8, \"5\": 11, \"6\": 8, \"7\": 12, \"8\": 12, \"9\": 12, \"10\": 2, \"11\": 2, \"12\": 2, \"13\": 2, \"14\": 2, \"15\": 8, \"16\": 0, \"17\": 0, \"18\": 8, \"19\": 12, \"20\": 8, \"21\": 0, \"22\": 0, \"23\": 6, \"24\": 6, \"25\": 13, \"26\": 13, \"27\": 13, \"28\": 13, \"29\": 13, \"30\": 13, \"31\": 13, \"32\": 13, \"33\": 13, \"34\": 13, \"35\": 13, \"36\": 13, \"37\": 13, \"38\": 13, \"39\": 13, \"40\": 13, \"41\": 13, \"42\": 13, \"43\": 13, \"44\": 13, \"45\": 13, \"46\": 13, \"47\": 13, \"48\": 13, \"49\": 13, \"50\": 13, \"51\": 8, \"52\": 4, \"53\": 4, \"54\": 8, \"55\": 4, \"56\": 4, \"57\": 4, \"58\": 4, \"59\": 4, \"60\": 4, \"61\": 4, \"62\": 4, \"63\": 4, \"64\": 4, \"65\": 4, \"66\": 4, \"67\": 4, \"68\": 4, \"69\": 13, \"70\": 13, \"71\": 13, \"72\": 13, \"73\": 13, \"74\": 13, \"75\": 0, \"76\": 0, \"77\": 0, \"78\": 0, \"79\": 0, \"80\": 0, \"81\": 0, \"82\": 0, \"83\": 0, \"84\": 0, \"85\": 0, \"86\": 0, \"87\": 0, \"88\": 0, \"89\": 0, \"90\": 0, \"91\": 0, \"92\": 13, \"93\": 13, \"94\": 13, \"95\": 13, \"96\": 13, \"97\": 13, \"98\": 13, \"99\": 13, \"100\": 13, \"101\": 13, \"102\": 13, \"103\": 13, \"104\": 13, \"105\": 13, \"106\": 13, \"107\": 13, \"108\": 13, \"109\": 13, \"110\": 13, \"111\": 13, \"112\": 13, \"113\": 13, \"114\": 13, \"115\": 13, \"116\": 13, \"117\": 0}\n",
        "\n",
        "name2label = {\n",
        "            \"total\": {\n",
        "                \"2\": [\"lung\"],\n",
        "                \"4\": [\"aorta\", \"artery\", \"atrial\", \"iliac\", \"vein\",\"vena\"],\n",
        "                \"6\": [\"kidney\"],\n",
        "                \"8\": [\"bowel\", \"colon\", \"esophagus\", \"gallbladder\", \"heart\", \"stomach\", \"trunk\", \"autochlon\", \"iliopsoas\", \"gluteus\"],\n",
        "                \"11\": [\"liver\"],\n",
        "                \"12\": [\"adrenal_gland\", \"duodenum\", \"pancreas\", \"spleen\"],\n",
        "                \"13\": [\"clavicula\", \"humerus\", \"rib_\", \"vertebrae_\", \"sacrum\", \"scapula\", \"sternum\", \"femur\", \"hip\", \"fibula\", \"tibia\", \"radius\", \"ulna\", \"carpal\", \"tarsal\", \"patella\"]\n",
        "            },\n",
        "            \"body\":{\n",
        "                \"bg\": 9,\n",
        "                \"skin\": 12,\n",
        "                \"fat\": 3,\n",
        "                \"muscle\": 8\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "palettedata = [ 0,0,0, 0,0,0, 220,30,30, 170,80,0, 0,170,0, 0,0,0, 0,175,20, 0,0,0, 0,170,190, 0,0,0, 0,0,0, 0,120,230, 115,65,200, 255,0,150] \n",
        "\n",
        "pointpalette = [torch.tensor([[0,0,0, 255],\n",
        "                            [0,0,0, 255],\n",
        "                            [220,30,30, 255],\n",
        "                            [170,80,0, 31], \n",
        "                            [0,170,0, 255], \n",
        "                            [0,0,0, 255],\n",
        "                            [0,175,20, 255], \n",
        "                            [0,0,0, 255],\n",
        "                            [0,170,190, 255], \n",
        "                            [0,0,0, 255],\n",
        "                            [0,0,0, 255], \n",
        "                            [0,120,230, 255], \n",
        "                            [115,65,200, 31], \n",
        "                            [255,0,150, 255]]),\n",
        "                [0, 0, 100000, 100000, 100000, 0, 100000, 0, 100000, 0, 0, 100000, 400000, 100000]]\n",
        "\n",
        "# from torch.nn import OptimizedModule\n",
        "def dict_2_map(d: dict[list[uint8], uint8]) -> list[list[uint8]]:\n",
        "    map = [[] for _ in range(15)]\n",
        "\n",
        "    for k, v in d.items():\n",
        "        int_k = uint8(k)\n",
        "        map[v].append(int_k)\n",
        "\n",
        "    return map\n",
        "\n",
        "def batched(self, iterable, n):\n",
        "    it = iter(iterable)\n",
        "    while batch := tuple(islice(it, n)):\n",
        "        yield batch\n",
        "\n",
        "# Save time by initializing predictors once, instead of for each task\n",
        "def initialize_predictors(device,\n",
        "                        folds: list = (0,)) -> dict:\n",
        "    \"\"\"\n",
        "    Initialize nnUNetPredictor instances for each segmentation task.\n",
        "\n",
        "    Args:\n",
        "        device (str): Device to run predictions on (device, 'cpu', 'mps').\n",
        "        use_folds (tuple): Fold indices to use for prediction.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary mapping task names to their respective nnUNetPredictor instances.\n",
        "    \"\"\"\n",
        "    # Define tasks\n",
        "    tasks = [(\"total\",\n",
        "            [291, 292, 293, 294, 295],\n",
        "            [\"Dataset291_TotalSegmentator_part1_organs_1559subj\",\n",
        "            \"Dataset292_TotalSegmentator_part2_vertebrae_1532subj\",\n",
        "            \"Dataset293_TotalSegmentator_part3_cardiac_1559subj\",\n",
        "            \"Dataset294_TotalSegmentator_part4_muscles_1559subj\",\n",
        "            \"Dataset295_TotalSegmentator_part5_ribs_1559subj\"],\n",
        "            [\"/v2.0.0-weights/Dataset291_TotalSegmentator_part1_organs_1559subj.zip\",\n",
        "            \"/v2.0.0-weights/Dataset292_TotalSegmentator_part2_vertebrae_1532subj.zip\",\n",
        "            \"/v2.0.0-weights/Dataset293_TotalSegmentator_part3_cardiac_1559subj.zip\",\n",
        "            \"/v2.0.0-weights/Dataset294_TotalSegmentator_part4_muscles_1559subj.zip\",\n",
        "            \"/v2.0.0-weights/Dataset295_TotalSegmentator_part5_ribs_1559subj.zip\"],\n",
        "            \"nnUNetTrainerNoMirroring\",\n",
        "            False),\n",
        "            (\"tissue_types\",\n",
        "            [481],\n",
        "            [\"Dataset481_tissue_1559subj\"],\n",
        "            [],\n",
        "            \"nnUNetTrainer\",\n",
        "            True),\n",
        "            (\"body\",\n",
        "            [299],\n",
        "            [\"Dataset299_body_1559subj\"],\n",
        "            [\"/v2.0.0-weights/Dataset299_body_1559subj.zip\"],\n",
        "            \"nnUNetTrainer\",\n",
        "            False)]\n",
        "\n",
        "    commercial_models_inv = {v: k for k, v in commercial_models.items()}\n",
        "    base_url = \"https://github.com/wasserth/TotalSegmentator/releases/download\"\n",
        "\n",
        "    # Get weights directory\n",
        "    weights_dir = get_weights_dir()\n",
        "    os.makedirs(weights_dir, exist_ok=True)\n",
        "\n",
        "    predictors = {}\n",
        "    for task_name, task_ids, paths, urls, trainer,with_license in tasks:\n",
        "        print(f\"INIT: {task_name} predictor\")\n",
        "        if with_license:\n",
        "            for i in range(len(task_ids)):\n",
        "                cfg_dataset = weights_dir / paths[i] / (trainer + '__nnUNetPlans__3d_fullres') / 'dataset.json'\n",
        "                if paths[i] not in os.listdir(weights_dir):\n",
        "                    download_model_with_license_and_unpack(commercial_models_inv[task_ids[i]], weights_dir)\n",
        "\n",
        "                    # # directly remaps label assignments, saving time\n",
        "                    # with open(cfg_dataset, mode='r') as f:\n",
        "                    #     data = json.load(f)\n",
        "\n",
        "                    # data['labels']['subcutaneous_fat'] = name2label['body']['fat']\n",
        "                    # data['labels']['torso_fat'] = name2label['body']['fat']\n",
        "                    # data['labels']['skeletal_muscle'] = name2label['body']['muscle']\n",
        "\n",
        "                    # with open(cfg_dataset, mode='w') as f:\n",
        "                    #     json.dump(data, f, indent=4)\n",
        "                    \n",
        "\n",
        "                # Initialize the predictor\n",
        "                predictor = nnUNetPredictor(\n",
        "                    tile_step_size=0.5,\n",
        "                    use_gaussian=True,\n",
        "                    use_mirroring=False,\n",
        "                    perform_everything_on_device=(device.type == device),\n",
        "                    device=device,\n",
        "                    verbose=True,\n",
        "                    allow_tqdm=True\n",
        "                )\n",
        "                # Initialize from the trained model folder\n",
        "                predictor.initialize_from_trained_model_folder(\n",
        "                    str(weights_dir / paths[i] / (trainer + \"__nnUNetPlans__3d_fullres\")),\n",
        "                    use_folds=folds,\n",
        "                    checkpoint_name='checkpoint_final.pth'\n",
        "                )\n",
        "\n",
        "                predictors[task_ids[i]] = predictor\n",
        "\n",
        "        else:\n",
        "            for i in range(len(urls)):\n",
        "                cfg_dataset = weights_dir / paths[i] / (trainer + '__nnUNetPlans__3d_fullres') / 'dataset.json'\n",
        "                if paths[i] not in os.listdir(weights_dir):\n",
        "                    download_url_and_unpack(base_url + urls[i], weights_dir)\n",
        "\n",
        "                    # # directly remaps label assignments, saving time\n",
        "                    # with open(cfg_dataset, mode='r') as f:\n",
        "                    #     data = json.load(f)\n",
        "\n",
        "                    # if task_name == 'total':\n",
        "                    #     for name, value in data['labels'].items():\n",
        "                    #         data['labels'][name] = total_lmap[str(value)]\n",
        "                    # else:\n",
        "                    #     # TODO we could merge body trunc with extremeties and get free skin generation for the whole body\n",
        "                    #     pass\n",
        "\n",
        "                    # with open(cfg_dataset, mode='w') as f:\n",
        "                    #     json.dump(data, f, indent=4)\n",
        "\n",
        "\n",
        "                # Initialize the predictor\n",
        "                predictor = nnUNetPredictor(\n",
        "                    tile_step_size=0.5,\n",
        "                    use_gaussian=True,\n",
        "                    # use_mirroring=(task_name!='total'),\n",
        "                    use_mirroring=False,\n",
        "                    perform_everything_on_device=(device.type == device),\n",
        "                    device=device,\n",
        "                    verbose=True,\n",
        "                    allow_tqdm=True\n",
        "                )\n",
        "                # Initialize from the trained model folder\n",
        "                predictor.initialize_from_trained_model_folder(\n",
        "                    str(weights_dir / paths[i] / (trainer + \"__nnUNetPlans__3d_fullres\")),\n",
        "                    use_folds=folds,\n",
        "                    checkpoint_name='checkpoint_final.pth'\n",
        "                )\n",
        "                predictors[task_ids[i]] = predictor\n",
        "\n",
        "    return predictors\n",
        "\n",
        "def bin_erosion(kernel:torch.Tensor, padded:torch.Tensor, ret:torch.Tensor):\n",
        "    # Assumes stacked 3d and no normalization needed\n",
        "    i, hdx, idx, jdx = cuda.grid(4)\n",
        "\n",
        "    # Run kernel\n",
        "    window = padded[i,\n",
        "                    hdx-int((kernel.shape[0]-1) / 2):hdx+int((kernel.shape[0]-1) / 2),\n",
        "                    idx-int((kernel.shape[0]-1) / 2):idx+int((kernel.shape[0]-1) / 2),\n",
        "                    jdx-int((kernel.shape[0]-1) / 2):jdx+int((kernel.shape[0]-1) / 2)]\n",
        "    # TODO: does this also get JITed?\n",
        "    match = torch.all(kernel == window)\n",
        "    ret[i, hdx, idx, jdx] = 1 if match else 0\n",
        "\n",
        "def bin_dilation(kernel:torch.Tensor, padded:torch.Tensor, ret:torch.Tensor):\n",
        "    # Assumes stacked 3d and no normalization needed\n",
        "    i, hdx, idx, jdx = cuda.grid(4)\n",
        "\n",
        "    # Run kernel\n",
        "    window = padded[i,\n",
        "                    hdx-int((kernel.shape[0]-1) / 2):hdx+int((kernel.shape[0]-1) / 2),\n",
        "                    idx-int((kernel.shape[0]-1) / 2):idx+int((kernel.shape[0]-1) / 2),\n",
        "                    jdx-int((kernel.shape[0]-1) / 2):jdx+int((kernel.shape[0]-1) / 2)]\n",
        "    # TODO: does this also get JITed?\n",
        "    match = torch.any(kernel == window)\n",
        "    ret[i, hdx, idx, jdx] = 1 if match else 0\n",
        "\n",
        "class CT2US(torch.nn.Module):\n",
        "    def seg_predictor(self, imgs, properties, task, resamp_thr):\n",
        "        return self.predictors[task].predict_from_list_of_npy_arrays(imgs,\n",
        "                                                    None,\n",
        "                                                    properties,\n",
        "                                                    None, 2, save_probabilities=False,\n",
        "                                                    num_processes_segmentation_export=resamp_thr)\n",
        "    \n",
        "    def seg_new(self, imgs, properties, task, resamp_thr):\n",
        "        return self.predictors[task].predict_from_data_iterator(\n",
        "                                        self.iterator(self.predictors[task], imgs, properties),\n",
        "                                        save_probabilities=False,\n",
        "                                        num_processes_segmentation_export=resamp_thr\n",
        "                                    )\n",
        "\n",
        "        # Does not work for some reason, totalsegmentator returns zeros instead of labels\n",
        "    def seg_old(self, imgs, properties, task, resamp_thr):\n",
        "        ret = []\n",
        "\n",
        "        # TODO Get list from \"total\" labels and find matches in totalsegmentator\n",
        "        roi = [l for _, l in self.name2label[\"total\"].items()]\n",
        "        roi = np.concatenate(roi).tolist()\n",
        "        if task == \"total\":\n",
        "            for img in imgs:\n",
        "                ret.append(np.asarray(ts.totalsegmentator(\n",
        "                                    input=img,\n",
        "                                    task=task,\n",
        "                                    nr_thr_resamp=resamp_thr\n",
        "                                    # roi_subset=roi\n",
        "                                ).dataobj, dtype=np.uint8))\n",
        "\n",
        "        else:\n",
        "            for img in imgs:\n",
        "                ret.append(np.asarray(ts.totalsegmentator(\n",
        "                                    input=img,\n",
        "                                    task=task,\n",
        "                                    nr_thr_resamp=resamp_thr\n",
        "                                ).dataobj, dtype=np.uint8))\n",
        "\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def __init__(self, method: str = 'old'):\n",
        "        super(CT2US, self).__init__()\n",
        "        methods = {'old', 'new', 'predictor'}\n",
        "        if not method in methods:\n",
        "            raise KeyError(f\"Method not supported, choose from {methods}\")\n",
        "        else:\n",
        "            self.method = method\n",
        "\n",
        "\n",
        "        self.device = device\n",
        "        if device.type == device and torch.cuda.is_available():\n",
        "            self.m = cp\n",
        "            self.dil_t = cuda.jit(bin_dilation)\n",
        "            self.er_t = cuda.jit(bin_erosion)\n",
        "            self.ops = cusci\n",
        "\n",
        "        else:\n",
        "            self.m = np\n",
        "            self.dil_t = njit(bin_dilation)\n",
        "            self.er_t = njit(bin_erosion)\n",
        "            self.ops = scipy.ndimage\n",
        "\n",
        "        if not method == 'old':\n",
        "            predictors = initialize_predictors(device=device, folds=[0])\n",
        "            self.predictors = predictors\n",
        "            self.predictor_keys = predictors.keys()\n",
        "\n",
        "        segmentator = {\n",
        "            # 'new': self.predict_tensor_iter,\n",
        "            'new': self.seg_new,\n",
        "            'predictor': self.seg_predictor,\n",
        "            'old': self.seg_old\n",
        "        }\n",
        "        self.segmentator = segmentator[method]\n",
        "\n",
        "        us = {\n",
        "            'new': self.to_us_sim_old,\n",
        "            'predictor': self.to_us_sim_old,\n",
        "            'old': self.to_us_sim_old\n",
        "        }\n",
        "        self.us = us[method]\n",
        "\n",
        "        composer = {\n",
        "            'new': self.stacked_assemble,\n",
        "            'predictor': self.stacked_assemble,\n",
        "            'old': self.assemble\n",
        "        }\n",
        "        self.composer = composer[method]\n",
        "\n",
        "        hparams = {\n",
        "            'debug' : False,\n",
        "            'device' : device\n",
        "        }\n",
        "\n",
        "        self.ultrasound_rendering = UltrasoundRendering(hparams, default_param=True)\n",
        "\n",
        "        self.total_lmap = total_lmap\n",
        "        self.name2label = name2label\n",
        "        self.tmap = dict_2_map(self.total_lmap)\n",
        "\n",
        "    def bin_dilation(self, imgs:torch.Tensor, kernel_size:int=3 ,iterations:int=1):\n",
        "        kernel = torch.ones((kernel_size, kernel_size, kernel_size), dtype=torch.uint8)\n",
        "        if imgs.is_cuda:\n",
        "            d_imgs = cuda.as_cuda_array(imgs.detach())\n",
        "            kernel = cuda.as_cuda_array(kernel.detach())\n",
        "            threadsperblock = (1, kernel_size, kernel_size, kernel_size)\n",
        "            blocks = (imgs.shape[0],\n",
        "                        np.ceil(imgs.shape[1] / threadsperblock[1]),\n",
        "                        np.ceil(imgs.shape[2] / threadsperblock[2]),\n",
        "                        np.ceil(imgs.shape[3] / threadsperblock[3]))\n",
        "            for _ in iterations:\n",
        "                ret = cuda.as_cuda_array(torch.zeros(imgs.shape, device=imgs.device).detach())\n",
        "                padded = cuda.as_cuda_array(\n",
        "                            imgs.to_padded_tensor(\n",
        "                                padding=0,\n",
        "                                output_size=(imgs.shape[0], imgs.shape[1] + 2, imgs.shape[2] + 2,imgs.shape[3] + 2)\n",
        "                            ).detach())\n",
        "                self._dil_cuda[blocks, threadsperblock](kernel, padded, ret)\n",
        "                d_imgs = ret\n",
        "        else:\n",
        "            d_imgs = imgs.detach().numpy()\n",
        "            kernel = kernel.detach()\n",
        "            for _ in iterations:\n",
        "                ret = torch.zeros(imgs.shape, device=imgs.device).detach()\n",
        "                padded = imgs.to_padded_tensor(\n",
        "                            padding=0,\n",
        "                            output_size=(imgs.shape[0], imgs.shape[1] + 2, imgs.shape[2] + 2,imgs.shape[3] + 2)\n",
        "                        ).detach()\n",
        "\n",
        "                self._dil_cpu(kernel, padded, ret)\n",
        "                d_imgs = ret\n",
        "\n",
        "        return d_imgs\n",
        "\n",
        "    def bin_erosion(self, imgs:torch.Tensor, kernel_size:int=3 ,iterations:int=1):\n",
        "        kernel = torch.ones((kernel_size, kernel_size, kernel_size), dtype=torch.uint8)\n",
        "        if imgs.is_cuda:\n",
        "            d_imgs = cuda.as_cuda_array(imgs.detach())\n",
        "            kernel = cuda.as_cuda_array(kernel.detach())\n",
        "            threadsperblock = (1, kernel_size, kernel_size, kernel_size)\n",
        "            blocks = (imgs.shape[0],\n",
        "                        np.ceil(imgs.shape[1] / threadsperblock[1]),\n",
        "                        np.ceil(imgs.shape[2] / threadsperblock[2]),\n",
        "                        np.ceil(imgs.shape[3] / threadsperblock[3]))\n",
        "            for _ in iterations:\n",
        "                ret = cuda.as_cuda_array(torch.zeros(imgs.shape, device=imgs.device).detach())\n",
        "                padded = cuda.as_cuda_array(\n",
        "                            imgs.to_padded_tensor(\n",
        "                                padding=0,\n",
        "                                output_size=(imgs.shape[0], imgs.shape[1] + 2, imgs.shape[2] + 2,imgs.shape[3] + 2)\n",
        "                            ).detach())\n",
        "                self._er_cuda[blocks, threadsperblock](kernel, padded, ret)\n",
        "                d_imgs = ret\n",
        "        else:\n",
        "            d_imgs = imgs.detach().numpy()\n",
        "            kernel = kernel.detach()\n",
        "            for _ in iterations:\n",
        "                ret = torch.zeros(imgs.shape, device=imgs.device).detach()\n",
        "                padded = imgs.to_padded_tensor(\n",
        "                            padding=0,\n",
        "                            output_size=(imgs.shape[0], imgs.shape[1] + 2, imgs.shape[2] + 2,imgs.shape[3] + 2)\n",
        "                        ).detach()\n",
        "\n",
        "                self._er_cpu(kernel, padded, ret)\n",
        "                d_imgs = ret\n",
        "\n",
        "        return d_imgs\n",
        "\n",
        "    # Adapted from nnUNetPredictor\n",
        "    def iterator(self,\n",
        "                predictor: nnUNetPredictor,\n",
        "                imgs: list[np.ndarray],\n",
        "                properties: list[dict]):\n",
        "\n",
        "        # MAYBE: look at data_iterators.preprocess_fromnpy_save_to_queue for vstack use for ROI foreground masking\n",
        "\n",
        "        # pp = predictor.get_data_iterator_from_raw_npy_data(\n",
        "        #     imgs,\n",
        "        #     properties\n",
        "        # )\n",
        "\n",
        "        # preprocessor = predictor.configuration_manager.preprocessor_class(verbose=predictor.verbose)\n",
        "\n",
        "        # properties = {key: [i[key] for i in properties] for key in properties[0]}\n",
        "\n",
        "        # data, seg = preprocessor.run_case_npy(\n",
        "        #                 np.stack(imgs),\n",
        "        #                 None,\n",
        "        #                 properties,\n",
        "        #                 predictor.plans_manager,\n",
        "        #                 predictor.configuration_manager,\n",
        "        #                 predictor.dataset_json\n",
        "        #             )\n",
        "\n",
        "        # pass\n",
        "\n",
        "        preprocessor = predictor.configuration_manager.preprocessor_class(verbose=predictor.verbose)\n",
        "        for a, p in zip(imgs, properties):\n",
        "            data, seg = preprocessor.run_case_npy(a,\n",
        "                                                  None,\n",
        "                                                  p,\n",
        "                                                  predictor.plans_manager,\n",
        "                                                  predictor.configuration_manager,\n",
        "                                                  predictor.dataset_json)\n",
        "            yield {'data': torch.from_numpy(data).contiguous().pin_memory(), 'data_properties': p, 'ofile': None}\n",
        "\n",
        "    def convert_logits_to_segmentation(self, prediction, properties, predictor):\n",
        "        spacing_transposed = [properties['spacing'][i] for i in predictor.plans_manager.transpose_forward]\n",
        "        current_spacing = predictor.configuration_manager.spacing if \\\n",
        "            len(predictor.configuration_manager.spacing) == \\\n",
        "            len(properties['shape_after_cropping_and_before_resampling']) else \\\n",
        "            [spacing_transposed[0], *predictor.configuration_manager.spacing]\n",
        "        predicted_logits = predictor.configuration_manager.resampling_fn_probabilities(predicted_logits,\n",
        "                                                properties['shape_after_cropping_and_before_resampling'],\n",
        "                                                current_spacing,\n",
        "                                                [properties['spacing'][i] for i in predictor.plans_manager.transpose_forward])\n",
        "        # return value of resampling_fn_probabilities can be ndarray or Tensor but that does not matter because\n",
        "        # apply_inference_nonlin will convert to torch\n",
        "        predicted_probabilities = predictor.label_manager.apply_inference_nonlin(predicted_logits)\n",
        "        del predicted_logits\n",
        "        segmentation = predictor.label_manager.convert_probabilities_to_segmentation(predicted_probabilities)\n",
        "\n",
        "        # put segmentation in bbox (revert cropping)\n",
        "        segmentation_reverted_cropping = np.zeros(properties['shape_before_cropping'],\n",
        "                                                dtype=np.uint8 if len(predictor.label_manager.foreground_labels) < 255 else np.uint16)\n",
        "        slicer = tuple([slice(*i) for i in properties['bbox_used_for_cropping']])\n",
        "        segmentation_reverted_cropping[slicer] = segmentation\n",
        "        del segmentation\n",
        "\n",
        "        pass\n",
        "\n",
        "    # Adapted from nnUNetPredictor\n",
        "    def predict_tensor_iter(self,\n",
        "                        data_iterator) -> list[torch.tensor]:\n",
        "\n",
        "        r = []\n",
        "        for preprocessed in data_iterator:\n",
        "            asm = []\n",
        "            data = preprocessed['data']\n",
        "\n",
        "            properties = preprocessed['data_properties']\n",
        "\n",
        "            for predictor in self.predictors.values():\n",
        "                old_threads = torch.get_num_threads()\n",
        "                # HYPERPARAMETER: number of threads to use for prediction\n",
        "                default_num_processes = 4\n",
        "                torch.set_num_threads(default_num_processes if default_num_processes < old_threads else old_threads)\n",
        "                prediction = None\n",
        "\n",
        "                for params in predictor.list_of_parameters:\n",
        "\n",
        "                    # messing with state dict names...\n",
        "                    # if not isinstance(predictor.network, OptimizedModule):\n",
        "                    #     predictor.network.load_state_dict(params)\n",
        "                    # else:\n",
        "                    #     predictor.network._orig_mod.load_state_dict(params)\n",
        "\n",
        "                    if prediction is None:\n",
        "                        prediction = predictor.predict_sliding_window_return_logits(data)\n",
        "                    else:\n",
        "                        prediction += predictor.predict_sliding_window_return_logits(data)\n",
        "\n",
        "                if len(predictor.list_of_parameters) > 1:\n",
        "                    prediction /= len(self.list_of_parameters)\n",
        "\n",
        "                prediction = self.convert_logits_to_segmentation(prediction, properties, predictor)\n",
        "\n",
        "            print(f'\\nDone with image of shape {data.shape}:')\n",
        "\n",
        "            # clear lru cache\n",
        "            compute_gaussian.cache_clear()\n",
        "            # clear device cache\n",
        "            if device.type == device:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            r.append()\n",
        "\n",
        "        return [i.get()[0] for i in r]\n",
        "\n",
        "    def assemble(self,\n",
        "                task:str,\n",
        "                segs:list[np.ndarray],\n",
        "                bases:list[np.ndarray],\n",
        "                prev:list[np.ndarray]) -> list[np.ndarray]:\n",
        "\n",
        "        print(f\"ASSEMBLY STARTED: {task}\")\n",
        "        # Process total segmentation\n",
        "\n",
        "        if task == 'total':\n",
        "            for j in range(len(segs)):\n",
        "                for i in range(len(self.tmap)):\n",
        "                    if len(self.tmap[i]) > 0:  # if there are any keys for this value\n",
        "                        a = self.m.where(self.m.isin(self.m.asarray(segs[j], dtype=self.m.uint8), self.m.array(self.tmap[i])), self.m.uint8(i), self.m.uint8(0))\n",
        "                        prev[j] += a\n",
        "\n",
        "        if task == 'tissue_types':\n",
        "            for j in range(len(segs)):\n",
        "                t = self.m.asarray(segs[j])\n",
        "\n",
        "                prev[j][t == 1] = self.m.uint8(self.name2label[\"body\"][\"fat\"])\n",
        "                prev[j][t == 2] = self.m.uint8(self.name2label[\"body\"][\"fat\"])\n",
        "\n",
        "        if task == 'body':\n",
        "            for j in range(len(segs)):\n",
        "                t = self.m.asarray(segs[j])\n",
        "\n",
        "                body = self.ops.binary_dilation(t == 1, iterations=1).astype(self.m.uint8)\n",
        "                body_inner = self.ops.binary_erosion(t, iterations=3, brute_force=True).astype(self.m.uint8)\n",
        "                \n",
        "                skin = body - body_inner\n",
        "                \n",
        "                # Segment by density\n",
        "                # Roughly the skin density range. Made large to make segmentation not have holes\n",
        "                # (0 to 250 would have many small holes in skin)\n",
        "                density_mask = (bases[j] > -200) & (bases[j] < 250)\n",
        "                skin[~density_mask] = 0\n",
        "\n",
        "                # Fill holes\n",
        "                # skin = binary_closing(skin, iterations=1)  # no real difference\n",
        "                # skin = binary_dilation(skin, iterations=1)  # not good\n",
        "\n",
        "                mask, _ = self.ops.label(skin)\n",
        "                counts = self.m.bincount(mask.flatten())  # number of pixels in each blob\n",
        "\n",
        "                # If only one blob (only background) abort because nothing to remove\n",
        "                if len(counts) > 1:\n",
        "                    remove = self.m.where((counts <= 10) | (counts > 30), True, False)\n",
        "                    remove_idx = self.m.nonzero(remove)[0]\n",
        "                    mask[self.m.isin(self.m.array(mask), remove_idx)] = 0\n",
        "                    mask[mask > 0] = 1\n",
        "\n",
        "                # Removing blobs\n",
        "                # End of snippet from totalsegmentator\n",
        "\n",
        "                dilation_kernel = self.m.ones(shape=(2, 2, 2))\n",
        "\n",
        "                skin = self.m.where(self.ops.binary_dilation(skin == 1, structure=dilation_kernel), self.m.uint8(1), self.m.uint8(0))\n",
        "\n",
        "                prev[j][skin == 1] = self.m.uint8(self.name2label[\"body\"][\"skin\"])\n",
        "\n",
        "                tmp = prev[j].copy()\n",
        "                prev[j][tmp == 0] = self.m.uint8(self.name2label[\"body\"][\"bg\"])\n",
        "\n",
        "        print(\"ASSEMBLY COMPLETED\")\n",
        "\n",
        "        del segs, bases\n",
        "\n",
        "        return prev\n",
        "\n",
        "    def stacked_assemble(self, task:str,\n",
        "                segs:list[np.ndarray],\n",
        "                stacked_bases:list[np.ndarray],\n",
        "                prev: list[np.ndarray]) -> list[np.ndarray]:\n",
        "\n",
        "        print(\"ASSEMBLY STARTED\")\n",
        "\n",
        "        # Process total segmentation\n",
        "        labels = prev\n",
        "\n",
        "        if task == \"total\":\n",
        "            stacked_totals = torch.stack([torch.as_tensor(segs[i], device=self.device) for i in range(stacked_bases.shape[0])], axis=0)\n",
        "            labels[prev != 0] = stacked_totals\n",
        "\n",
        "        elif task == \"tissue_types\":\n",
        "            stacked_tissues = torch.stack([torch.as_tensor(segs[i], device=self.device) for i in range(stacked_bases.shape[0])], axis=0)\n",
        "            labels[stacked_tissues != 0] = stacked_tissues\n",
        "\n",
        "        elif task == \"body\":\n",
        "            stacked_outers = torch.stack([torch.as_tensor(segs[i], device=self.device, dtype=torch.uint8) for i in range(stacked_bases.shape[0])], axis=0)\n",
        "\n",
        "            # Adapted code snippet from totalsegmentator\n",
        "            body = self.bin_dilation(stacked_outers == 1, kernel_size=3, iterations=1).astype(torch.uint8)\n",
        "            body_inner = self.bin_erosion(stacked_outers == 1, kernel_size=3, iterations=3).astype(torch.uint8)\n",
        "            skin = body - body_inner\n",
        "\n",
        "            # Segment by density\n",
        "            # Roughly the skin density range. Made large to make segmentation not have holes\n",
        "            # (0 to 250 would have many small holes in skin)\n",
        "            density_mask = (stacked_bases > -200) & (stacked_bases < 250)\n",
        "            skin[~density_mask] = 0\n",
        "\n",
        "            # Fill holes\n",
        "            # skin = binary_closing(skin, iterations=1)  # no real difference\n",
        "            # skin = binary_dilation(skin, iterations=1)  # not good\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                mask, _ = cusci.label(skin)\n",
        "            else:\n",
        "                mask, _ = scipy.ndimage.label(skin)\n",
        "\n",
        "            counts = torch.bincount(mask.flatten())  # number of pixels in each blob\n",
        "\n",
        "            # If only one blob (only background) abort because nothing to remove\n",
        "            if len(counts) > 1:\n",
        "                remove = torch.where((counts <= 10) | (counts > 30), True, False)\n",
        "                remove_idx = torch.nonzero(remove)[0]\n",
        "                mask[torch.isin(mask, remove_idx)] = 0\n",
        "                mask[mask > 0] = 1\n",
        "\n",
        "            # Removing blobs\n",
        "            # End of snippet from totalsegmentator\n",
        "            mask = torch.where(self.bin_dilation(mask == 1, kernel_size=3, iterations = 2), np.uint8(1), np.uint8(0))\n",
        "\n",
        "            labels[mask == 1] = np.uint8(self.name2label[\"body\"][\"skin\"])\n",
        "\n",
        "            tmp = labels.copy()\n",
        "            labels[tmp == 0] = np.uint8(self.name2label[\"body\"][\"bg\"])\n",
        "\n",
        "        print(\"ASSEMBLY COMPLETED\")\n",
        "        del segs, bases\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def to_us_sim_new(self, segs:list[np.ndarray], properties:dict, dest_us: list[str], step_size:int) -> list[list[np.ndarray]]:\n",
        "        print(\"US SIMULATION STARTED\")\n",
        "\n",
        "        hparams = {\n",
        "            'debug' : False,\n",
        "            'device' : device\n",
        "        }\n",
        "\n",
        "        us_r = UltrasoundRendering(params=hparams, default_param=True).to(hparams['device'])\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Resize([380, 380], transforms.InterpolationMode.NEAREST),\n",
        "                    transforms.CenterCrop((256)),\n",
        "                ])\n",
        "\n",
        "        results = []\n",
        "        for i in range(len(segs)):\n",
        "            us_images = []\n",
        "            labelmap = segs[i].get()\n",
        "            dest = pthlib(dest_us[i]).joinpath(\"slice_\")\n",
        "            os.makedirs(dest.parent, exist_ok=True)\n",
        "\n",
        "            for slice_idx in range(0, labelmap.shape[2], step_size):\n",
        "                slice_data = labelmap[:, :, slice_idx].astype('int64')\n",
        "                labelmap_slice = transform(slice_data).squeeze()\n",
        "\n",
        "                us_image = us_r(labelmap_slice)\n",
        "                us_images.append(us_image.cpu().numpy())\n",
        "\n",
        "                us_image_pil = transforms.ToPILImage()(us_image.cpu().squeeze())\n",
        "                us_image_pil.save(f\"{dest}_{slice_idx}.png\")\n",
        "\n",
        "            results.append(us_images)\n",
        "\n",
        "        print(\"US SIMULATION COMPLETED\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def to_us_sim_old(self, segs:list[np.ndarray], properties:dict, dest_us: list[str],  step_size:int) -> list[tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
        "        print(\"US SIMULATION STARTED\")\n",
        "\n",
        "        hparams = {\n",
        "            'debug' : False,\n",
        "            'device' : self.device\n",
        "        }\n",
        "\n",
        "        l_dict = {\n",
        "            2:'lung',\n",
        "            3:'fat',\n",
        "            4:'vessel',\n",
        "            6:'kidney',\n",
        "            8:'muscle',\n",
        "            11:'liver',\n",
        "            12:'soft tissue',\n",
        "            13:'bone'\n",
        "        }\n",
        "        us_r = UltrasoundRendering(params=hparams, default_param=True).to(device)\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Resize([380, 380], transforms.InterpolationMode.NEAREST),\n",
        "                    transforms.CenterCrop((256)),\n",
        "                ])\n",
        "\n",
        "        us_imgs = []\n",
        "        pcd_base = []\n",
        "\n",
        "        for i in tqdm.tqdm(range(len(segs)), desc=\"Rendering\"):\n",
        "            # p = properties[i][\"spacing\"]\n",
        "            warped = []\n",
        "            us_slices = []\n",
        "            if self.m == cp:\n",
        "                labelmap = segs[i].get()\n",
        "            else:\n",
        "                labelmap = segs[i]\n",
        "\n",
        "            dest = pthlib(dest_us[i]).joinpath(\"slice_\")\n",
        "            os.makedirs(dest.parent, exist_ok=True)\n",
        "\n",
        "            for slice_idx in tqdm.tqdm(range(0, labelmap.shape[2], step_size), desc=\"US slice rendering\"):\n",
        "                slice_data = labelmap[:, :, slice_idx].astype('int64')\n",
        "                labelmap_slice = transform(slice_data).squeeze()\n",
        "\n",
        "                us_slice = us_r(labelmap_slice)\n",
        "                if self.m == np:\n",
        "                    us_slices.append(us_slice.cpu().numpy())\n",
        "                else:\n",
        "                    us_slices.append(us_slice)    \n",
        "\n",
        "                us_image_pil = transforms.ToPILImage()(us_slice.cpu().squeeze())\n",
        "                us_image_pil.save(f\"{dest}_{slice_idx}.png\")\n",
        "\n",
        "                # Warp slice to match US and create individual label maps\n",
        "                \n",
        "                if self.m == np:\n",
        "                    temp = us_r.warp_img(labelmap_slice.cuda(self.device)).cpu().numpy()\n",
        "                else:\n",
        "                    temp = self.m.asarray(us_r.warp_img(labelmap_slice.cuda(self.device)))\n",
        "\n",
        "                a = self.m.fliplr(self.m.asarray(temp.copy()).transpose(1, 0))\n",
        " \n",
        "                warped.append([(\n",
        "                                    self.ops.binary_fill_holes(\n",
        "                                        # scipy.ndimage.binary_erosion(\n",
        "                                            self.ops.binary_dilation(\n",
        "                                                self.ops.binary_closing(a==tag, iterations=3), iterations=2)),\n",
        "                                                # scipy.ndimage.binary_closing(a==tag, iterations=3), iterations=2), iterations=1)),\n",
        "                                    # scipy.ndimage.binary_erosion(scipy.ndimage.binary_dilation(a==tag, iterations=2), iterations=2),\n",
        "                                    l_dict[tag]\n",
        "                                ) for tag in [2, 3, 4, 6, 8, 11, 12, 13]])\n",
        " \n",
        "                del temp, a\n",
        "\n",
        "            torch.set_default_device(device)\n",
        "            temp = torch.as_tensor(labelmap)\n",
        "\n",
        "            list_pos = []\n",
        "            list_color = []\n",
        "            for i in tqdm.tqdm(range(len(pointpalette[1])), desc=\"Pointcloud rendering\"):\n",
        "                if pointpalette[1][i] != 0:\n",
        "                    base = torch.zeros_like(temp)\n",
        "                    base[temp == i] = 1\n",
        "                    tmp = base.nonzero()\n",
        "\n",
        "                    idx_list = list(range(tmp.shape[0]))\n",
        "                    select_idx = self.m.random.choice(idx_list, size=pointpalette[1][i])\n",
        "                    list_pos.append(tmp[select_idx])\n",
        "                    \n",
        "                    tmp = torch.zeros((pointpalette[1][i], 4), dtype=torch.uint8)\n",
        "                    tmp[:,...] = torch.as_tensor(pointpalette[0][i])\n",
        "                    list_color.append(tmp)\n",
        "\n",
        "            pcd_base.append([list_pos, list_color, temp.shape])\n",
        "\n",
        "            us_imgs.append(us_slices)\n",
        "            # warped_labels.append(warped)\n",
        "            \n",
        "\n",
        "        print(\"US SIMULATION COMPLETED\")\n",
        "\n",
        "        return us_imgs, warped, pcd_base\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                    imgs: list[nifti1.Nifti1Image|np.ndarray],\n",
        "                    properties:list[dict],\n",
        "                    dest_label: list[str],\n",
        "                    dest_us: list[str],\n",
        "                    step_size: int,\n",
        "                    save_labels: bool,\n",
        "                ) -> list[list[np.ndarray]]:\n",
        "\n",
        "        if not self.method == 'old':\n",
        "            bases = torch.stack([\n",
        "                        torch.as_tensor(\n",
        "                            img,\n",
        "                            dtype=torch.float32,\n",
        "                            device=self.device\n",
        "                        ).squeeze() for img in imgs]\n",
        "                    ).cuda(self.device)\n",
        "\n",
        "            f_labels = torch.stack([\n",
        "                            torch.zeros(\n",
        "                                bases[i].shape,\n",
        "                                dtype=torch.uint8,\n",
        "                                device=self.device\n",
        "                            ) for i in range(bases.shape[0])],\n",
        "                            axis=0\n",
        "                        ).cuda(self.device)\n",
        "        else:\n",
        "            bases = [self.m.array(img.dataobj, dtype=self.m.float32) for img in imgs]\n",
        "            f_labels = [self.m.zeros(bases[idx].shape, dtype=self.m.uint8) for idx in range(len(imgs))]\n",
        "\n",
        "        if self.method == \"old\":\n",
        "            tasks = [\"total\", \"tissue_types\", \"body\"]\n",
        "        else:\n",
        "            tasks = list(self.predictor_keys)\n",
        "\n",
        "        print(\"SEGMENTATING:\")\n",
        "\n",
        "        tmp = []\n",
        "        for idx in tqdm.tqdm(range(len(tasks)), desc=\"Segmentating batch\"):\n",
        "\n",
        "            tmp.append(self.segmentator(imgs, properties, tasks[idx], 4))\n",
        "            print(\"SEG DONE!\")\n",
        "\n",
        "        if self.method == 'old':\n",
        "            for idx in tqdm.tqdm(range(len(tasks)), desc=\"Composing into suitable intermediate\"):\n",
        "                f_labels = self.composer(tasks[idx], tmp[idx], bases, f_labels)\n",
        "\n",
        "        us_imgs, warped_labels, pcdb = self.us(f_labels.copy(), properties, dest_us, step_size)\n",
        "\n",
        "        if save_labels:\n",
        "            for idx in tqdm.tqdm(range(len(f_labels)), desc=\"Saving labels\"):\n",
        "                if self.m == cp:\n",
        "                    SimpleITKIO().write_seg(f_labels[idx].get().transpose(2, 1, 0), dest_label[idx], properties[idx])\n",
        "                else:\n",
        "                    SimpleITKIO().write_seg(f_labels[idx].transpose(2, 1, 0), dest_label[idx], properties[idx])\n",
        "                    \n",
        "                print(f\"SAVED TO '{dest_label[idx]}'\")\n",
        "\n",
        "        t = []\n",
        "        for f in tqdm.tqdm(f_labels, desc=\"Generating warped labels for annotated view\"):\n",
        "            temp = []\n",
        "            if self.m == cp:\n",
        "                labels = f.get()\n",
        "            else:\n",
        "                labels = f\n",
        "                \n",
        "            for arr in list(np.flip(labels.copy().transpose(2, 1, 0), 2))[::step_size]:\n",
        "                img = Image.fromarray(arr)\n",
        "                img.putpalette(palettedata *16)\n",
        "                temp.append(img)\n",
        "            t.append(temp)\n",
        "\n",
        "        return [str(pthlib(d).name) for d in dest_label], us_imgs, warped_labels, t, pcdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "id": "BOiTb1SiK5Lk"
      },
      "outputs": [],
      "source": [
        "def collate_tensor(data):\n",
        "    imgs, properties, dest_labels, dest_us = zip(*data)\n",
        "\n",
        "    return torch.from_numpy(imgs), properties, dest_labels, dest_us\n",
        "\n",
        "def collate_list(data):\n",
        "    imgs, properties, dest_labels, dest_us = zip(*data)\n",
        "    return imgs, properties, dest_labels, dest_us\n",
        "\n",
        "# import pandas as pd\n",
        "class CTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir:str, method:str=\"old\", annotations_file:str=None, resample:float=1.5):\n",
        "        self.device = device\n",
        "        self.method = method\n",
        "        if device.type == device:\n",
        "            self.m = cp\n",
        "\n",
        "        else:\n",
        "            self.m = np\n",
        "\n",
        "        self.collate_fn = collate_list\n",
        "\n",
        "        if not isinstance(img_dir, pthlib):\n",
        "            img_dir = pthlib(img_dir)\n",
        "\n",
        "        self.resample = resample\n",
        "\n",
        "        self.img_dir = img_dir\n",
        "        self.annotations_file = annotations_file\n",
        "\n",
        "        l = {\"predictor\" : self.load_bases, \"new\" : self.load_bases, \"old\": self.load_bases}\n",
        "        self.load = l[method]\n",
        "\n",
        "        # r = {\"predictor\" : self.resampler_old, \"new\" : self.resampler_new, \"old\": None}\n",
        "        r = {\"predictor\" : None, \"new\" : None, \"old\": None}\n",
        "        self.resampler = r[method]\n",
        "\n",
        "        if self.annotations_file is not None:\n",
        "            # TODO IMPLEMENT LOADING FROM CSV\n",
        "            # self.img_paths = pd.read_csv(annotations_file)\n",
        "            pass\n",
        "        else:\n",
        "            self.img_paths = glob.glob(f\"{str(self.img_dir)}/*.nii.gz\")\n",
        "            self.img_paths = [(pth,\n",
        "                                pth.replace(\".nii.gz\", \"_label.nii.gz\").replace(\"/imgs/\", \"/labels/\"),\n",
        "                                pth.replace(\".nii.gz\", \"_us\").replace(\"/imgs/\", \"/us/\")\n",
        "                                ) for pth in self.img_paths]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def load_bases(self, pths: str) -> tuple[np.ndarray, dict]:\n",
        "        img, prop = SimpleITKIO().read_images(image_fnames=[join(pths)])\n",
        "        return (img, prop)\n",
        "\n",
        "    def resampler_new(self, img) -> np.ndarray:\n",
        "        spacing = np.array(img[1][\"spacing\"])\n",
        "\n",
        "        new_spacing = np.array(self.resample)\n",
        "\n",
        "        if self.m.array_equal(spacing, new_spacing):\n",
        "            resampled_img = (img)\n",
        "        else:\n",
        "            zoom = spacing / new_spacing\n",
        "\n",
        "        data = self.m.array(img[0], self.m.float32)\n",
        "\n",
        "        new_shape = (np.array(data.shape) * zoom).round().astype(self.m.int32)\n",
        "\n",
        "        if self.device.type == device:\n",
        "            resampled_img = (self.m.array(resize(data, output_shape=new_shape, order=3, mode=\"edge\", anti_aliasing=False)), img[1], img[2], img[3])\n",
        "        else:\n",
        "            resampled_img = (self.m.array(rs.resample_img(data, zoom)), img[1], img[2], img[3])\n",
        "\n",
        "        return resampled_img\n",
        "\n",
        "    def resampler_old(self, img) -> np.ndarray:\n",
        "        spacing = np.array(img[0].header.get_zooms())\n",
        "        new_spacing = np.array(self.resample)\n",
        "\n",
        "        if self.m.array_equal(spacing, new_spacing):\n",
        "            resampled_img = (img)\n",
        "        else:\n",
        "            zoom = spacing / new_spacing\n",
        "\n",
        "        data = self.m.array(img[0].get_fdata(), self.m.float32)\n",
        "\n",
        "        new_shape = (np.array(data.shape) * zoom).round().astype(self.m.int32)\n",
        "\n",
        "        if self.device.type == device:\n",
        "            resampled_img = (self.m.array(resize(data, output_shape=new_shape, order=3, mode=\"edge\", anti_aliasing=False)), {\"spacing\": spacing}, img[1], img[2])\n",
        "        else:\n",
        "            resampled_img = (self.m.array(rs.resample_img(data, zoom)), {\"spacing\": spacing}, img[1], img[2])\n",
        "\n",
        "        return resampled_img\n",
        "\n",
        "    # TODO CLOSEST CANONICAL through transforms?\n",
        "    def __getitem__(self, idx): # -> list[tuple[np.ndarray, dict, str, str]]:\n",
        "        todo = self.img_paths[idx]\n",
        "        img, prop = self.load(todo[0])\n",
        "        dest_label = todo[1]\n",
        "        dest_us = todo[2]\n",
        "\n",
        "        # nnunet predictors require shape (1, x, y, z)\n",
        "        if self.method == \"predictor\":\n",
        "            ret = (img[0][None,...], prop, dest_label, dest_us)\n",
        "        elif self.method == \"new\":\n",
        "            ret = (img, prop, dest_label, dest_us)\n",
        "        elif self.method == \"old\":\n",
        "            affine = np.eye(4)\n",
        "            affine[:3, :3] = np.array(prop[\"sitk_stuff\"][\"direction\"]).reshape(3, 3) * prop[\"sitk_stuff\"][\"spacing\"]\n",
        "            affine[:3, 3] = prop[\"sitk_stuff\"][\"origin\"]\n",
        "\n",
        "            ret = (nifti1.Nifti1Image(img[0].transpose(2, 1, 0), affine=affine), prop, dest_label, dest_us)\n",
        "\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mWpZV-caaT3"
      },
      "source": [
        "# Acquire samples for demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-8Ddh3PbEJE",
        "outputId": "98a2a15f-f408-4d16-b8b6-829ebcc079ba"
      },
      "outputs": [],
      "source": [
        "todo_dir = pthlib.joinpath(this_folder, \"sample\")\n",
        "todo_dir.mkdir(exist_ok=True)\n",
        "\n",
        "if os.listdir(todo_dir) == []:\n",
        "    print(\"Downloading sample data\")\n",
        "    !wget -O /CT2US/sample/sample.zip \"https://www.dropbox.com/scl/fi/y44t2wu7eyg0t3fpknoxv/img.zip?rlkey=5uza6964xrrtffzfc7w977m3z&st=q6u2hzkh&dl=1\"\n",
        "    !unzip '/CT2US/sample/sample.zip' -d '/CT2US/sample'\n",
        "    !rm '/CT2US/sample/sample.zip'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run this for the actual UI\n",
        "\n",
        "There is still some work to be done here, especially a method choice after the new pipeline components are done and a preview of the slices (former by applying affine to labelmap and erasing background label, as to create a volume that can be directly displayed by gradio i.e. .obj file and latter via a simple slice selection and image 2d image preview). Generating a volume to allow for easy identification of slice location still wip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_dir = this_folder / \"imgs\"\n",
        "label_dir = this_folder / \"labels\"\n",
        "us_dir = this_folder / \"us\"\n",
        "gen_dir = this_folder / \"gen\"\n",
        "\n",
        "os.makedirs(img_dir, exist_ok=True)\n",
        "os.makedirs(label_dir, exist_ok=True)\n",
        "os.makedirs(us_dir, exist_ok=True)\n",
        "os.makedirs(gen_dir, exist_ok=True)\n",
        "\n",
        "os.environ[\"GRADIO_ALLOWED_PATHS\"]=str(this_folder)\n",
        "\n",
        "axis_sample_size = 20000\n",
        "axis_points_rgba = [255,255,255,255]\n",
        "\n",
        "def add_axis_pcd(points, colors, shape, y) -> tri.PointCloud:\n",
        "    pcd_points = points.copy()\n",
        "    pcd_colors = colors.copy()\n",
        "\n",
        "    axis = torch.zeros(shape, device=device)\n",
        "    axis[:,:,y] = 1\n",
        "    tmp = axis.nonzero()\n",
        "    \n",
        "    idx_list = list(range(tmp.shape[0]))\n",
        "    select_idx = np.random.choice(idx_list, size=axis_sample_size)\n",
        "    pcd_points.append(tmp[select_idx])\n",
        "\n",
        "    tmp = torch.zeros((axis_sample_size, 4), dtype=torch.uint8, device=device)\n",
        "    tmp[:,...] = torch.as_tensor(axis_points_rgba)\n",
        "    pcd_colors.append(tmp)\n",
        "\n",
        "    point_pos = torch.concat(pcd_points)\n",
        "    colors = torch.concat(pcd_colors)\n",
        "\n",
        "    point_pos = point_pos.float()\n",
        "\n",
        "    point_displacement = torch.rand(point_pos.shape).to(device)\n",
        "    point_pos += point_displacement\n",
        "\n",
        "    shape = torch.FloatTensor(np.array(shape)).to(device)\n",
        "    point_pos /= shape\n",
        "    point_pos = point_pos - .5\n",
        "\n",
        "    pcd = tri.PointCloud(point_pos.cpu(), colors.cpu()).apply_transform(\n",
        "                    np.dot(\n",
        "                        tri.transformations.rotation_matrix(np.pi, [1, 0, 0]),\n",
        "                        tri.transformations.rotation_matrix(np.pi/2, [0, -1, 0])\n",
        "                    )\n",
        "            )\n",
        "\n",
        "    return pcd\n",
        "\n",
        "\n",
        "with gr.Blocks() as ct_2_us: \n",
        "    with gr.Row():\n",
        "        files = gr.State({})\n",
        "        us_list = gr.State({})\n",
        "        warped_list = gr.State({})\n",
        "        label_list = gr.State({})\n",
        "        pcdb_list = gr.State({})\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            ct_imgs = gr.Files(file_types=['.nii', '.nii.gz'], type='filepath', label=\"Select CT images\", interactive=True, file_count='multiple')\n",
        "            step_size = gr.Slider(label=\"Slicing step interval\", minimum=1, maximum=20, value=2, step=1, interactive=True)       \n",
        "\n",
        "            with gr.Row():\n",
        "                btn = gr.Button(\"Generate\")\n",
        "                reset = gr.Button(\"Reset\")\n",
        "\n",
        "            @gr.on([reset.click, ct_2_us.load], inputs=None, outputs=[files, us_list, warped_list, label_list, pcdb_list])\n",
        "            def reset_all():\n",
        "                for f in label_dir.glob('*.nii.gz'):\n",
        "                    os.remove(f)\n",
        "                for f in img_dir.glob('*.nii.gz'):\n",
        "                    os.remove(f)\n",
        "                for f in gen_dir.glob('*.glb'):\n",
        "                    os.remove(f)\n",
        "                for f in glob.glob(f\"{us_dir}/*\"):\n",
        "                    shutil.rmtree(f, ignore_errors=True)\n",
        "                try:\n",
        "                    os.remove(f\"{this_folder}/results.zip\")\n",
        "                except:\n",
        "                    print(\"No need to delete results\")\n",
        "                return {}, {}, {}, {}, {}\n",
        "\n",
        "            # gr.Examples([item for item in reduce(lambda result, x: result + [subset + [x] for subset in result], examples, [[]]) if len(item)>0], ct_imgs)\n",
        "            # gr.Examples(\n",
        "            #                 examples=[[str(path)] for path in sorted(pthlib(this_folder / 'sample').glob('**/*.nii'))]\n",
        "            #                         + [[str(path)] for path in sorted(pthlib(this_folder / 'sample').glob('**/*.nii.gz'))], \n",
        "            #                 inputs=[ct_imgs],\n",
        "            #                 cache_examples=False,\n",
        "            #                 label='Sample CT volumes',\n",
        "            #                 examples_per_page=10\n",
        "            #             )\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    sample_in = gr.Dropdown(\n",
        "                                                choices=[i+1 for i in range(len(glob.glob(str(this_folder / 'sample' / '*.nii.gz'))))], \n",
        "                                                label='Amount of samples to randomly select',\n",
        "                                                info='Used for demo with no input'    \n",
        "                                            )\n",
        "                    seg_method = gr.Radio(choices=[\"predictor\", \"new\", \"old\"], value=\"old\", label=\"Segmentation method\", interactive=True)\n",
        "                    us_method = gr.Radio(choices=[\"lotus\"], value=\"lotus\", label=\"US rendering method\", interactive=True)\n",
        "        \n",
        "        with gr.Column(scale=2):\n",
        "\n",
        "            with gr.Tab(label='Preview'):\n",
        "                note = gr.Markdown(label=\"status\", value=\"Generate US images first through the input tab\")\n",
        "\n",
        "                img_idx = gr.State(0)\n",
        "                slice_idx = gr.State(0)\n",
        "\n",
        "                @gr.render(inputs=[files, us_list, warped_list, label_list, pcdb_list, step_size], triggers=[us_list.change])\n",
        "                def dynamic(fl, us, warped, ll, pcdb, step):            \n",
        "                    with gr.Column():\n",
        "                        if len(us) > 0:\n",
        "                            dropdown = gr.Dropdown(choices=[(f[0], n) for n, f in fl.items()], label='Select image to preview', value=0)\n",
        "                            slider = gr.Slider(minimum=0, maximum=len(warped[0]) - 1, step=step, label='Slice selection', value=0)\n",
        "                            \n",
        "                            iden = lambda x: x\n",
        "\n",
        "                            slider.release(fn=iden, inputs=[slider], outputs=[slice_idx])\n",
        "                            dropdown.select(fn=iden, inputs=[dropdown], outputs=[img_idx])\n",
        "\n",
        "                            # States for results and dropdown selection\n",
        "                            with gr.Column():\n",
        "                                # \"original\"\n",
        "                                with gr.Row():\n",
        "                                    base = gr.Image(\n",
        "                                                label='US slice',\n",
        "                                                value=np.asarray(us[img_idx.value][slice_idx.value], dtype=np.float32),\n",
        "                                                height=300\n",
        "                                            )\n",
        "                                    \n",
        "                                    label_preview = gr.Image(\n",
        "                                                        label='Label slice',\n",
        "                                                        value=ll[0][0],\n",
        "                                                        type='pil',\n",
        "                                                        height=300\n",
        "                                                    )\n",
        "                                # BOTTOM ROW -> annotation and 3ddw\n",
        "                                with gr.Row():\n",
        "                                    comp = gr.AnnotatedImage(\n",
        "                                                value=(np.asarray(us[img_idx.value][slice_idx.value], dtype=np.float32), warped[0][0]),\n",
        "                                                height=300\n",
        "                                            )\n",
        "                                    \n",
        "                                    volume_preview = gr.Model3D(clear_color=(0, 0, 0, 1), label=\"Label map view\", value=str(gen_dir / \"current_pcd.glb\"), height=300)\n",
        "                                    \n",
        "                                def route(x, y):\n",
        "                                    b = np.asarray(us[x][y], dtype=np.float32)\n",
        "                                    w = warped[x][y]\n",
        "                                    l = ll[x][y]\n",
        "                                    add_axis_pcd(pcdb[x][0], pcdb[x][1], pcdb[x][2], y * step).export(str(gen_dir / \"current_pcd.glb\"))\n",
        "                                    \n",
        "                                    p = str(gen_dir / \"current_pcd.glb\")\n",
        "                                    return (b, w), b, l, p, y if y <= len(us[x]) else 0, \\\n",
        "                                        gr.Slider(minimum=0, maximum=len(warped[x]) - 1, step=step, label='Slice selection', value=y if y <= len(us[x]) else 0)\n",
        "\n",
        "                                    \n",
        "                                gr.on(triggers=[img_idx.change, slice_idx.change],\n",
        "                                        fn=route,\n",
        "                                        inputs=[img_idx, slice_idx],\n",
        "                                        outputs=[comp, base, label_preview, volume_preview, slice_idx, slider]\n",
        "                                        # outputs=[comp, base, label_preview]\n",
        "                                    )\n",
        "                            \n",
        "            with gr.Tab(label='Download'):\n",
        "                download = gr.DownloadButton(label=\"\", visible=False)\n",
        "                # TODOallow for picking of specific results, potentially also generate point clouds into files and allow user to download them?\n",
        "                @gr.render(inputs=[files, us_list, warped_list, label_list, pcdb_list, step_size], triggers=[us_list.change])\n",
        "                def dynamic(fl, us, warped, ll, pcdb, step):\n",
        "                    descr = gr.Markdown(label=\"This can be used to adjust contents of results.zip\")\n",
        "                    configs = gr.CheckboxGroup(choices=[\"Save labels\"], value=[\"Save labels\"], label=\"Options\", interactive=True)\n",
        "                    filename_in = gr.Textbox(label=\"Filename for result zip\", value=\"results\")\n",
        "\n",
        "                    r = []\n",
        "                    r.append(label_dir.glob('*.nii.gz'))\n",
        "                    r.append(us_dir.glob('**/*.png'))\n",
        "\n",
        "\n",
        "                    rezip = gr.Button(\"Reassemble results.zip\")\n",
        "\n",
        "                    @gr.on(rezip.click, inputs=[configs, filename_in], outputs=[download, descr])\n",
        "                    def rezip_files(save_configs, name, r=r):\n",
        "                        with ZipFile(f\"{this_folder}/results.zip\", 'w') as zipObj:\n",
        "                            for f in r[0]:\n",
        "                                zipObj.write(f, os.path.relpath(f, str(this_folder)))\n",
        "                                os.remove(f)\n",
        "                            for f in r[1]:\n",
        "                                zipObj.write(f, os.path.relpath(f, str(this_folder)))\n",
        "\n",
        "                            for f in glob.glob(f\"{us_dir}/*\"):\n",
        "                                shutil.rmtree(f, ignore_errors=True)\n",
        "\n",
        "                        return f\"{this_folder}/{name}.zip\", \"Results have been rezipped\"\n",
        "\n",
        "                def start(ct, step, method, method_us, fl_s, us_s, warped_s, ll_s, pcdb_s, nr_samples, progress=gr.Progress(track_tqdm=True)):\n",
        "                    if ct == None:\n",
        "                        ct = glob.glob(str(this_folder / 'sample' / '*.nii.gz'))\n",
        "                        ct = [this_folder / 'sample' / f for f in ct]\n",
        "\n",
        "                    ct = random.sample(ct, k=nr_samples)\n",
        "\n",
        "                    for f in ct:\n",
        "                        shutil.copyfile(f, img_dir / f.name)\n",
        "                        shutil.rmtree(f, ignore_errors=True)\n",
        "\n",
        "                    local_dataset = CTDataset(\n",
        "                        img_dir=img_dir,\n",
        "                        method=method,\n",
        "                        resample=None\n",
        "                    )\n",
        "                    batch_size = 1\n",
        "\n",
        "                    ct_dataloader = DataLoader(local_dataset, batch_size=batch_size, collate_fn=local_dataset.collate_fn)\n",
        "\n",
        "                    ct2us = CT2US(method=method)\n",
        "\n",
        "                    fl = []\n",
        "                    us = []\n",
        "                    warped = []\n",
        "                    ll = []\n",
        "                    pcdb = []\n",
        "\n",
        "                    for data in progress.tqdm(ct_dataloader, desc=\"Processing batches\"):\n",
        "                        imgs, properties, dest_labels, dest_us = data\n",
        "                        \n",
        "                        # TODO: Last parameter saves labels. This is intentended to allow for more download options and storing of other intermediary results that might be of interest \n",
        "                        n, u, w, l, b = ct2us(imgs, properties, dest_labels, dest_us, step, True)\n",
        "                        \n",
        "                        fl.append(n)\n",
        "                        us.append(*u)\n",
        "                        warped.append(w)\n",
        "                        ll.append(*l)\n",
        "                        pcdb.append(*b)\n",
        "                    \n",
        "\n",
        "                    add_axis_pcd(pcdb[0][0], pcdb[0][1], pcdb[0][2], 0).export(str(gen_dir / \"current_pcd.glb\"))\n",
        "\n",
        "                    fl_s.update(enumerate(fl))\n",
        "                    us_s.update(enumerate(us))\n",
        "                    warped_s.update(enumerate(warped))\n",
        "                    ll_s.update(enumerate(ll))\n",
        "                    pcdb_s.update(enumerate(pcdb))\n",
        "\n",
        "                    return gr.DownloadButton(label=\"Download results as zip\", visible=True, value=f\"{this_folder}/results.zip\"), \\\n",
        "                            fl_s, \\\n",
        "                            us_s, \\\n",
        "                            warped_s, \\\n",
        "                            ll_s, \\\n",
        "                            pcdb_s, \\\n",
        "                            gr.Markdown(value=\"Status\", height=30)\n",
        "                                \n",
        "                btn.click(\n",
        "                            fn=lambda x: gr.Markdown(label=\"Status\", value=\"\", height=80), \n",
        "                            inputs=btn, \n",
        "                            outputs=note\n",
        "                        ).success(\n",
        "                            fn=start, \n",
        "                            inputs=[ct_imgs, step_size, seg_method, us_method, files, us_list, warped_list, label_list, pcdb_list, sample_in], \n",
        "                            outputs=[download, files, us_list, warped_list, label_list, pcdb_list, note]\n",
        "                        ).success(\n",
        "                            fn=lambda x: gr.Markdown(label=\"\", value=\"\", height=0, visible=False), \n",
        "                            inputs=btn, \n",
        "                            outputs=note\n",
        "                        )\n",
        "                \n",
        "            # def run(progress=gr.Progress(track_tqdm=True)):\n",
        "                \n",
        "            # btn.click(fn=run,\n",
        "            #             inputs=None,\n",
        "            #             outputs=[files, us_imgs, warped_labels, labels, note])\n",
        "\n",
        "ct_2_us.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPKOXOBjyKehHbXIHBR+Pc0",
      "gpuType": "T4",
      "include_colab_link": true,
      "mount_file_id": "1Fe4FNFHx23VIfet1AkxD8J95Mfqn_XlC",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "ct2us",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
