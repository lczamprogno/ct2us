{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lczamprogno/ct2us/blob/main/CT2US.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzmsGLmOGUvw"
      },
      "source": [
        "# CT2US\n",
        "\n",
        "This tool is intended to automate the generation of simulated ultrasound image and label pairs from ct volumes (.nii/.nii.gz).\n",
        "\n",
        "---\n",
        "\n",
        "### Purpose\n",
        "Intended to be capable of supplementing datasets for ultrasound image labeling.\n",
        "\n",
        "## Expandability\n",
        "Image generation process is very dependant on tissue attenuation, so specialized US renderers would be necessary/ideal to expand this tool to work on other body parts. For this purpose, much of the following code has hence been designed with modularity as a core goal, so that new methods can be added/replaced, as for example the segmentation quality or speed could have a significant impact on overall results.\n",
        "\n",
        "---\n",
        "\n",
        "## Current use:\n",
        "![](https://drive.google.com/uc?export=view&id=16LZ1vCoQ48w1Xn8lBJ1ZrsRe0ngEgbjB)  \n",
        "\n",
        "## Further goals:\n",
        "- code for two alternate optimized segmentation pipelines is still being developed\n",
        "  - one focusing on avoiding internal totalsegmentator steps being saved to memory [ ]\n",
        "  - another further optimizes by properly using gpu and cpu acceleration. [ ]\n",
        "\n",
        "- Throughout the process of acquiring the necessary information for the rendering - and even for visualization - could be a useful resource. With that in mind, enabling saving these intermediary results is a goal, albeit one that has yet to be fully achieved. The download tab is intended to serve the purpose of adjusting saved information. [ ]\n",
        "\n",
        "- Improved version of the totalsegmentator nnunet is still WIP. Once that is taken care of, pluging this in the pipeline with the stacked assemble should yield a significant speed up. [ ]\n",
        "\n",
        "- Currently there is some support for cpu, but this script is mainly designed with gpu acceleration in mind. For cpu optimization, the label_map dict needs to be used to gather all the label names which are contained. This list then can be used by just plugging it in the totalsegmentator function as a parameter, and the built in ROI should yield faster results. [ ]\n",
        "\n",
        "- There seemed to be no significant improvement through using the ROI feature in the gpu version. Here, it is important to note that allocation of gpu memory and copying data over has a significant memory cost, which is actually one of the main improvements of the new totalsegmentator pipeline implemented. [ ]\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU0pI50IcNbI"
      },
      "source": [
        "This needs to be run once and then the session needs to be restarted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "8-C6Ik3Z48SC",
        "outputId": "e23d6c87-994d-427b-f5eb-0adefbd0d0af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colab\n",
            "  Downloading colab-1.13.5.tar.gz (567 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/567.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.7/567.7 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting totalsegmentator\n",
            "  Downloading TotalSegmentator-2.6.0-py3-none-any.whl.metadata (913 bytes)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (0.61.0)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (13.3.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting torchio\n",
            "  Downloading torchio-0.20.4-py3-none-any.whl.metadata (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cucim\n",
            "  Downloading cucim-23.10.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: bokeh>=3.1.0 in /usr/local/lib/python3.11/dist-packages (3.6.3)\n",
            "Collecting di\n",
            "  Downloading di-0.79.2-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.15.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Collecting trimesh[easy]\n",
            "  Downloading trimesh-4.6.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: torch<2.6.0,>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from totalsegmentator) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from totalsegmentator) (1.26.4)\n",
            "Collecting SimpleITK (from totalsegmentator)\n",
            "  Downloading SimpleITK-2.4.1-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: nibabel>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from totalsegmentator) (5.3.2)\n",
            "Requirement already satisfied: tqdm>=4.45.0 in /usr/local/lib/python3.11/dist-packages (from totalsegmentator) (4.67.1)\n",
            "Collecting xvfbwrapper (from totalsegmentator)\n",
            "  Downloading xvfbwrapper-0.2.9.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nnunetv2>=2.2.1 (from totalsegmentator)\n",
            "  Downloading nnunetv2-2.5.2.tar.gz (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.6/200.6 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dicom2nifti (from totalsegmentator)\n",
            "  Downloading dicom2nifti-2.5.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from totalsegmentator) (17.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from totalsegmentator) (2.32.3)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba) (0.44.0)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x) (0.8.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.6.0,>=2.1.2->totalsegmentator) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.6.0,>=2.1.2->totalsegmentator) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.6.0,>=2.1.2->totalsegmentator) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.6.0,>=2.1.2->totalsegmentator) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.6.0,>=2.1.2->totalsegmentator) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<2.6.0,>=2.1.2->totalsegmentator)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<2.6.0,>=2.1.2->totalsegmentator)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<2.6.0,>=2.1.2->totalsegmentator)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.6.0,>=2.1.2->totalsegmentator)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<2.6.0,>=2.1.2->totalsegmentator)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<2.6.0,>=2.1.2->totalsegmentator)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<2.6.0,>=2.1.2->totalsegmentator)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<2.6.0,>=2.1.2->totalsegmentator)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<2.6.0,>=2.1.2->totalsegmentator)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.6.0,>=2.1.2->totalsegmentator) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.6.0,>=2.1.2->totalsegmentator) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<2.6.0,>=2.1.2->totalsegmentator)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.6.0,>=2.1.2->totalsegmentator) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.6.0,>=2.1.2->totalsegmentator) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.6.0,>=2.1.2->totalsegmentator) (1.3.0)\n",
            "Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.11/dist-packages (from torchio) (1.2.18)\n",
            "Requirement already satisfied: humanize>=0.1 in /usr/local/lib/python3.11/dist-packages (from torchio) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from torchio) (24.2)\n",
            "Requirement already satisfied: rich>=10 in /usr/local/lib/python3.11/dist-packages (from torchio) (13.9.4)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.11/dist-packages (from torchio) (1.13.1)\n",
            "Requirement already satisfied: typer>=0.1 in /usr/local/lib/python3.11/dist-packages (from torchio) (0.15.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from cucim) (8.1.8)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from cucim) (0.4)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1.0) (1.3.1)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1.0) (2.2.2)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1.0) (6.0.2)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1.0) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1.0) (2025.1.0)\n",
            "Collecting graphlib2<0.5.0,>=0.4.1 (from di)\n",
            "  Downloading graphlib2-0.4.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.0 (from gradio)\n",
            "  Downloading gradio_client-1.7.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.9.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.0->gradio) (14.2)\n",
            "Collecting colorlog (from trimesh[easy])\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting manifold3d>=2.3.0 (from trimesh[easy])\n",
            "  Downloading manifold3d-3.0.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from trimesh[easy]) (5.2.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from trimesh[easy]) (5.3.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from trimesh[easy]) (4.23.0)\n",
            "Collecting svg.path (from trimesh[easy])\n",
            "  Downloading svg.path-6.3-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pycollada (from trimesh[easy])\n",
            "  Downloading pycollada-0.8.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.1/108.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from trimesh[easy]) (75.1.0)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (from trimesh[easy]) (2.0.7)\n",
            "Collecting xxhash (from trimesh[easy])\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting rtree (from trimesh[easy])\n",
            "  Downloading Rtree-1.3.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting embreex (from trimesh[easy])\n",
            "  Downloading embreex-2.17.7.post6-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (2.9 kB)\n",
            "Collecting xatlas (from trimesh[easy])\n",
            "  Downloading xatlas-0.0.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting vhacdx (from trimesh[easy])\n",
            "  Downloading vhacdx-0.0.8.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting mapbox_earcut>=1.0.2 (from trimesh[easy])\n",
            "  Downloading mapbox_earcut-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2->torchio) (1.17.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel>=2.3.0->totalsegmentator) (6.5.2)\n",
            "Collecting acvl-utils<0.3,>=0.2 (from nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading acvl_utils-0.2.3.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dynamic-network-architectures<0.4,>=0.3.1 (from nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading dynamic_network_architectures-0.3.1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting batchgenerators>=0.25 (from nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading batchgenerators-0.25.1.tar.gz (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from nnunetv2>=2.2.1->totalsegmentator) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from nnunetv2>=2.2.1->totalsegmentator) (0.25.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from nnunetv2>=2.2.1->totalsegmentator) (0.20.3)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.11/dist-packages (from nnunetv2>=2.2.1->totalsegmentator) (2025.1.10)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from nnunetv2>=2.2.1->totalsegmentator) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from nnunetv2>=2.2.1->totalsegmentator) (0.13.2)\n",
            "Collecting imagecodecs (from nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading imagecodecs-2024.12.30-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting yacs (from nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Collecting batchgeneratorsv2>=0.2 (from nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading batchgeneratorsv2-0.2.1.tar.gz (34 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from nnunetv2>=2.2.1->totalsegmentator) (0.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->bokeh>=3.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->bokeh>=3.1.0) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->bokeh>=3.1.0) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10->torchio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10->torchio) (2.18.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.1->torchio) (1.5.4)\n",
            "Collecting pydicom>=2.2.0 (from dicom2nifti->totalsegmentator)\n",
            "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting python-gdcm (from dicom2nifti->totalsegmentator)\n",
            "  Downloading python_gdcm-3.0.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->trimesh[easy]) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->trimesh[easy]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->trimesh[easy]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->trimesh[easy]) (0.22.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->totalsegmentator) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->totalsegmentator) (2.3.0)\n",
            "Collecting connected-components-3d (from acvl-utils<0.3,>=0.2->nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading connected_components_3d-3.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
            "Requirement already satisfied: blosc2>=3.0.0b4 in /usr/local/lib/python3.11/dist-packages (from acvl-utils<0.3,>=0.2->nnunetv2>=2.2.1->totalsegmentator) (3.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.25->nnunetv2>=2.2.1->totalsegmentator) (1.0.0)\n",
            "Collecting unittest2 (from batchgenerators>=0.25->nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading unittest2-1.1.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.25->nnunetv2>=2.2.1->totalsegmentator) (3.5.0)\n",
            "Collecting fft-conv-pytorch (from batchgeneratorsv2>=0.2->nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading fft_conv_pytorch-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10->torchio) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2->bokeh>=3.1.0) (1.17.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.19.3->nnunetv2>=2.2.1->totalsegmentator) (2.37.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2>=2.2.1->totalsegmentator) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2>=2.2.1->totalsegmentator) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2>=2.2.1->totalsegmentator) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2>=2.2.1->totalsegmentator) (3.2.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->nnunetv2>=2.2.1->totalsegmentator) (1.4.2)\n",
            "Requirement already satisfied: ndindex in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2>=2.2.1->totalsegmentator) (1.9.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2>=2.2.1->totalsegmentator) (1.1.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2>=2.2.1->totalsegmentator) (2.10.2)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2>=2.2.1->totalsegmentator) (9.0.0)\n",
            "Collecting argparse (from unittest2->batchgenerators>=0.25->nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting traceback2 (from unittest2->batchgenerators>=0.25->nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading traceback2-1.4.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting linecache2 (from traceback2->unittest2->batchgenerators>=0.25->nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading linecache2-1.0.0-py2.py3-none-any.whl.metadata (1000 bytes)\n",
            "Downloading TotalSegmentator-2.6.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m828.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading torchio-0.20.4-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.8/175.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cucim-23.10.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading di-0.79.2-py3-none-any.whl (24 kB)\n",
            "Downloading gradio-5.15.0-py3-none-any.whl (57.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.0-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.9/321.9 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphlib2-0.4.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.1/223.1 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading manifold3d-3.0.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mapbox_earcut-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.0/97.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m119.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading SimpleITK-2.4.1-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading dicom2nifti-2.5.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading embreex-2.17.7.post6-cp311-cp311-manylinux_2_28_x86_64.whl (17.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.2/17.2 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading Rtree-1.3.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (543 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m543.2/543.2 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading svg.path-6.3-py2.py3-none-any.whl (16 kB)\n",
            "Downloading trimesh-4.6.1-py3-none-any.whl (707 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m707.0/707.0 kB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vhacdx-0.0.8.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xatlas-0.0.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imagecodecs-2024.12.30-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_gdcm-3.0.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading connected_components_3d-3.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fft_conv_pytorch-1.2.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: nnunetv2, pycollada, xvfbwrapper, acvl-utils, batchgenerators, batchgeneratorsv2, dynamic-network-architectures\n",
            "  Building wheel for nnunetv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nnunetv2: filename=nnunetv2-2.5.2-py3-none-any.whl size=270697 sha256=47b46abb83032339df2ad5de0f55d38ab2e642467fa180881551cc331c2d6895\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/2f/aa/55b9ffc0ca0154839ab38feeeef5c20f829d305c8b4999e4a0\n",
            "  Building wheel for pycollada (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycollada: filename=pycollada-0.8-py3-none-any.whl size=127514 sha256=60c41e7ae9a22f970bbc648138b2012fc0a6c3ac6ea1d7b3cf26a7156acad35a\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/f0/6c/2fd64355f3a11cb6673fd92e0417c5ed5bf37a796d9c180562\n",
            "  Building wheel for xvfbwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xvfbwrapper: filename=xvfbwrapper-0.2.9-py3-none-any.whl size=5010 sha256=f97a83a44e46e26f1d30076baf87a66cf072a6ec94a361be99d397b557552ba7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/5c/40/945b2f824d1770d8b13123f3f598562449be8fc8aa51acfff2\n",
            "  Building wheel for acvl-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for acvl-utils: filename=acvl_utils-0.2.3-py3-none-any.whl size=25723 sha256=eeccb44598e48a16825d8914b37c2fecccc25dcf7de1578b05f2757b80620fd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/ee/dd/e00ec00cf61a47207a0b5e36f32dfc68e809fd071cc33b8672\n",
            "  Building wheel for batchgenerators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for batchgenerators: filename=batchgenerators-0.25.1-py3-none-any.whl size=93088 sha256=83ec224ebe0fa917a528ae4cfd5f544a3587c313676a4624edbda36b92b620b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/11/c7/fadca30e054c602093ffe36ba8a2f0a87dd2f86ac75191d3ed\n",
            "  Building wheel for batchgeneratorsv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for batchgeneratorsv2: filename=batchgeneratorsv2-0.2.1-py3-none-any.whl size=45187 sha256=0c6951e54b96aecdafc3feea1de6ca5895d5af6a30391f9c92b6fbdef38cfebe\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/db/4e/a245424d70ed2f5c6d6446a63e2ddf7a8f0f0293896698391d\n",
            "  Building wheel for dynamic-network-architectures (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dynamic-network-architectures: filename=dynamic_network_architectures-0.3.1-py3-none-any.whl size=30046 sha256=35a19cbecf8bed14bd7ba65b4fb8e7f7d1b398090c499d36dc8134d4980d03e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/8f/23/133ba252665b6f93abdbb294b323cc8fec041be83e4d22b701\n",
            "Successfully built nnunetv2 pycollada xvfbwrapper acvl-utils batchgenerators batchgeneratorsv2 dynamic-network-architectures\n",
            "Installing collected packages: xvfbwrapper, SimpleITK, pydub, linecache2, argparse, yacs, xxhash, xmltodict, xatlas, vhacdx, uvicorn, trimesh, traceback2, tomlkit, svg.path, semantic-version, ruff, rtree, python-multipart, python-gdcm, pydicom, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, markupsafe, mapbox_earcut, manifold3d, imagecodecs, graphlib2, ffmpy, embreex, connected-components-3d, colorlog, aiofiles, unittest2, starlette, pycollada, nvidia-cusparse-cu12, nvidia-cudnn-cu12, dicom2nifti, di, cucim, safehttpx, nvidia-cusolver-cu12, gradio-client, fastapi, batchgenerators, gradio, torchio, fft-conv-pytorch, dynamic-network-architectures, acvl-utils, batchgeneratorsv2, nnunetv2, totalsegmentator\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed SimpleITK-2.4.1 acvl-utils-0.2.3 aiofiles-23.2.1 argparse-1.4.0 batchgenerators-0.25.1 batchgeneratorsv2-0.2.1 colorlog-6.9.0 connected-components-3d-3.22.0 cucim-23.10.0 di-0.79.2 dicom2nifti-2.5.1 dynamic-network-architectures-0.3.1 embreex-2.17.7.post6 fastapi-0.115.8 ffmpy-0.5.0 fft-conv-pytorch-1.2.0 gradio-5.15.0 gradio-client-1.7.0 graphlib2-0.4.7 imagecodecs-2024.12.30 linecache2-1.0.0 manifold3d-3.0.1 mapbox_earcut-1.0.3 markupsafe-2.1.5 nnunetv2-2.5.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pycollada-0.8 pydicom-3.0.1 pydub-0.25.1 python-gdcm-3.0.24.1 python-multipart-0.0.20 rtree-1.3.0 ruff-0.9.6 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.45.3 svg.path-6.3 tomlkit-0.13.2 torchio-0.20.4 totalsegmentator-2.6.0 traceback2-1.4.0 trimesh-4.6.1 unittest2-1.1.0 uvicorn-0.34.0 vhacdx-0.0.8.post1 xatlas-0.0.9 xmltodict-0.14.2 xvfbwrapper-0.2.9 xxhash-3.5.0 yacs-0.1.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              },
              "id": "879afc11e5db4f379b5065e4ddfa8c17"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%pip install colab\n",
        "%pip install totalsegmentator numba cupy-cuda12x torchvision xmltodict torchio cucim \"bokeh>=3.1.0\" di gradio pathlib trimesh[easy]\n",
        "\n",
        "#numpy-stl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y52AvG-caZ6u"
      },
      "source": [
        "# Classes and methods are here\n",
        "\n",
        "## Run this block\n",
        "\n",
        "IMPORTANT: Acquire a totalsegmentator key (https://backend.totalsegmentator.com/license-academic/) and set google colab secret as shown:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1dTIOm2P2soqp3COY0XCPTHZrRKalkBYN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "9HH9ibpeNCYr",
        "outputId": "748465d3-a4ab-4c77-8206-142e59584fc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "from pathlib import PosixPath as pthlib\n",
        "from zipfile import ZipFile\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from math import pi\n",
        "import tqdm\n",
        "\n",
        "from itertools import islice\n",
        "import glob\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "from numpy import uint8\n",
        "\n",
        "\n",
        "from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO\n",
        "from nibabel import nifti1\n",
        "\n",
        "import totalsegmentator.python_api as ts\n",
        "from totalsegmentator.config import setup_nnunet, setup_totalseg, set_config_key, get_weights_dir\n",
        "\n",
        "this_folder = pthlib(\"../CT2US\").resolve()\n",
        "\n",
        "sys.path.append(this_folder)\n",
        "ts_cfg_path = pthlib.joinpath(this_folder, \".totalsegmentator\")\n",
        "ts_cfg_path.mkdir(exist_ok=True, parents=True)\n",
        "os.environ[\"TOTALSEG_HOME_DIR\"] = str(ts_cfg_path)\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    license = userdata.get('license_key')\n",
        "except ImportError as e:\n",
        "    print(e)\n",
        "\n",
        "from totalsegmentator.libs import download_model_with_license_and_unpack, download_url_and_unpack\n",
        "from totalsegmentator import resampling as rs\n",
        "from totalsegmentator.map_to_binary import commercial_models\n",
        "\n",
        "setup_nnunet()\n",
        "setup_totalseg()\n",
        "\n",
        "ts.set_license_number(license)\n",
        "\n",
        "from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
        "from nnunetv2.inference.sliding_window_prediction import compute_gaussian\n",
        "\n",
        "from cucim.skimage.transform import resize\n",
        "from batchgenerators.utilities.file_and_folder_operations import join\n",
        "\n",
        "from numba import jit, njit, cuda\n",
        "\n",
        "try:\n",
        "    import cupy as cp\n",
        "    import cupyx.scipy.ndimage as cusci\n",
        "except ImportError:\n",
        "    print(\"Error loading cupy and cusci, GPU not available?\")\n",
        "\n",
        "import scipy.ndimage\n",
        "import scipy\n",
        "\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from torch import device\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import gradio as gr\n",
        "import trimesh as tri\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda\", 0) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "torch.set_default_device(device)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Kqfa3ZcNbM"
      },
      "source": [
        "US slice simulation code(from https://github.com/danivelikova/lotus/blob/main/models/us_rendering_model.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRLJBwammqoh",
        "outputId": "2a03f49f-f47c-4c19-fad2-a075635b9fb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_device.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# 2 - lung; 3 - fat; 4 - vessel; 6 - kidney; 8 - muscle; 9 - background; 11 - liver; 12 - soft tissue; 13 - bone;\n",
        "# Default Parameters from: https://github.com/Blito/burgercpp/blob/master/examples/ircad11/liver.scene , labels 8, 9 and 12 approximated from other labels\n",
        "\n",
        "                     # indexes:           2       3     4     6     8      9     11    12    13\n",
        "acoustic_imped_def_dict = torch.tensor([0.0004, 1.38, 1.61,  1.62, 1.62,  0.3,  1.65, 1.63, 7.8], requires_grad=True).to(device=device)    # Z in MRayl\n",
        "attenuation_def_dict =    torch.tensor([1.64,   0.63, 0.18,  1.0,  1.09, 0.54,  0.7,  0.54, 5.0], requires_grad=True).to(device=device)    # alpha in dB cm^-1 at 1 MHz\n",
        "mu_0_def_dict =           torch.tensor([0.78,   0.5,  0.001, 0.45,  0.45,  0.3,  0.4, 0.45, 0.78], requires_grad=True).to(device=device) # mu_0 - scattering_mu   mean brightness\n",
        "mu_1_def_dict =           torch.tensor([0.56,   0.5,  0.0,   0.6,  0.64,  0.2,  0.8,  0.64, 0.56], requires_grad=True).to(device=device) # mu_1 - scattering density, Nr of scatterers/voxel\n",
        "sigma_0_def_dict =        torch.tensor([0.1,    0.0,  0.01,  0.3,  0.1,   0.0,  0.14, 0.1,  0.1], requires_grad=True).to(device=device) # sigma_0 - scattering_sigma - brightness std\n",
        "\n",
        "\n",
        "alpha_coeff_boundary_map = 0.1\n",
        "beta_coeff_scattering = 10  #100 approximates it closer\n",
        "TGC = 8\n",
        "CLAMP_VALS = True\n",
        "\n",
        "\n",
        "def gaussian_kernel(size: int, mean: float, std: float):\n",
        "    d1 = torch.distributions.Normal(mean, std)\n",
        "    d2 = torch.distributions.Normal(mean, std*3)\n",
        "    vals_x = d1.log_prob(torch.arange(-size, size+1, dtype=torch.float32)).exp()\n",
        "    vals_y = d2.log_prob(torch.arange(-size, size+1, dtype=torch.float32)).exp()\n",
        "\n",
        "    gauss_kernel = torch.einsum('i,j->ij', vals_x, vals_y)\n",
        "\n",
        "    return gauss_kernel / torch.sum(gauss_kernel).reshape(1, 1)\n",
        "\n",
        "g_kernel = gaussian_kernel(3, 0., 0.5)\n",
        "g_kernel = torch.tensor(g_kernel[None, None, :, :], dtype=torch.float32).to(device=device)\n",
        "\n",
        "\n",
        "class UltrasoundRendering(torch.nn.Module):\n",
        "    def __init__(self, params, default_param=False):\n",
        "        super(UltrasoundRendering, self).__init__()\n",
        "        self.params = params\n",
        "\n",
        "        if default_param:\n",
        "            self.acoustic_impedance_dict = acoustic_imped_def_dict.detach().clone()\n",
        "            self.attenuation_dict = attenuation_def_dict.detach().clone()\n",
        "            self.mu_0_dict = mu_0_def_dict.detach().clone()\n",
        "            self.mu_1_dict = mu_1_def_dict.detach().clone()\n",
        "            self.sigma_0_dict = sigma_0_def_dict.detach().clone()\n",
        "\n",
        "        else:\n",
        "            self.acoustic_impedance_dict = torch.nn.Parameter(acoustic_imped_def_dict)\n",
        "            self.attenuation_dict = torch.nn.Parameter(attenuation_def_dict)\n",
        "\n",
        "            self.mu_0_dict = torch.nn.Parameter(mu_0_def_dict)\n",
        "            self.mu_1_dict = torch.nn.Parameter(mu_1_def_dict)\n",
        "            self.sigma_0_dict = torch.nn.Parameter(sigma_0_def_dict)\n",
        "\n",
        "        self.labels = [\"lung\", \"fat\", \"vessel\", \"kidney\", \"muscle\", \"background\", \"liver\", \"soft tissue\", \"bone\"]\n",
        "\n",
        "        self.attenuation_medium_map, self.acoustic_imped_map, self.sigma_0_map, self.mu_1_map, self.mu_0_map  = ([] for i in range(5))\n",
        "\n",
        "\n",
        "    def map_dict_to_array(self, dictionary, arr):\n",
        "        mapping_keys = torch.tensor([2, 3, 4, 6, 8, 9, 11, 12, 13], dtype=torch.long).to(device=device)\n",
        "        keys = torch.unique(arr).to(device=device)\n",
        "\n",
        "        index = torch.where(mapping_keys[None, :] == keys[:, None])[1]\n",
        "        values = torch.gather(dictionary, dim=0, index=index)\n",
        "        values = values.to(device=device)\n",
        "        # values.register_hook(lambda grad: print(grad))    # check the gradient during training\n",
        "\n",
        "        mapping = torch.zeros(keys.max().item() + 1).to(device=device)\n",
        "        mapping[keys] = values\n",
        "        return mapping[arr]\n",
        "\n",
        "\n",
        "    def plot_fig(self, fig, fig_name, grayscale):\n",
        "        save_dir='results_test/'\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "\n",
        "        plt.clf()\n",
        "\n",
        "        if torch.is_tensor(fig):\n",
        "            fig = fig.cpu().detach().numpy()\n",
        "\n",
        "        if grayscale:\n",
        "            plt.imshow(fig, cmap='gray', vmin=0, vmax=1, interpolation='none', norm=None)\n",
        "        else:\n",
        "            plt.imshow(fig, interpolation='none', norm=None)\n",
        "        plt.axis('off')\n",
        "        plt.savefig(save_dir + fig_name + '.png', bbox_inches='tight',transparent=True, pad_inches=0)\n",
        "\n",
        "\n",
        "    def clamp_map_ranges(self):\n",
        "        self.attenuation_medium_map = torch.clamp(self.attenuation_medium_map, 0, 10)\n",
        "        self.acoustic_imped_map = torch.clamp(self.acoustic_imped_map, 0, 10)\n",
        "        self.sigma_0_map = torch.clamp(self.sigma_0_map, 0, 1)\n",
        "        self.mu_1_map = torch.clamp(self.mu_1_map, 0, 1)\n",
        "        self.mu_0_map = torch.clamp(self.mu_0_map, 0, 1)\n",
        "\n",
        "\n",
        "    def rendering(self, H, W, z_vals=None, refl_map=None, boundary_map=None):\n",
        "\n",
        "        dists = torch.abs(z_vals[..., :-1, None] - z_vals[..., 1:, None])     # dists.shape=(W, H-1, 1)\n",
        "        dists = dists.squeeze(-1)                                             # dists.shape=(W, H-1)\n",
        "        dists = torch.cat([dists, dists[:, -1, None]], dim=-1)                # dists.shape=(W, H)\n",
        "\n",
        "        attenuation = torch.exp(-self.attenuation_medium_map * dists)\n",
        "        attenuation_total = torch.cumprod(attenuation, dim=1, dtype=torch.float32, out=None)\n",
        "\n",
        "        gain_coeffs = np.linspace(1, TGC, attenuation_total.shape[1])\n",
        "        gain_coeffs = np.tile(gain_coeffs, (attenuation_total.shape[0], 1))\n",
        "        gain_coeffs = torch.tensor(gain_coeffs).to(device=device)\n",
        "        attenuation_total = attenuation_total * gain_coeffs     # apply TGC\n",
        "\n",
        "        reflection_total = torch.cumprod(1. - refl_map * boundary_map, dim=1, dtype=torch.float32, out=None)\n",
        "        reflection_total = reflection_total.squeeze(-1)\n",
        "        reflection_total_plot = torch.log(reflection_total + torch.finfo(torch.float32).eps)\n",
        "\n",
        "        texture_noise = torch.randn(H, W, dtype=torch.float32).to(device=device)\n",
        "        scattering_probability = torch.randn(H, W, dtype=torch.float32).to(device=device)\n",
        "\n",
        "        scattering_zero = torch.zeros(H, W, dtype=torch.float32).to(device=device)\n",
        "\n",
        "        z = self.mu_1_map - scattering_probability\n",
        "        sigmoid_map = torch.sigmoid(beta_coeff_scattering * z)\n",
        "\n",
        "        # approximating  Eq. (4) to be differentiable:\n",
        "        # where(scattering_probability <= mu_1_map,\n",
        "        #                     texture_noise * sigma_0_map + mu_0_map,\n",
        "        #                     scattering_zero)\n",
        "        scatterers_map =  (sigmoid_map) * (texture_noise * self.sigma_0_map + self.mu_0_map) + (1 -sigmoid_map) * scattering_zero   # Eq. (6)\n",
        "\n",
        "        psf_scatter_conv = torch.nn.functional.conv2d(input=scatterers_map[None, None, :, :], weight=g_kernel, stride=1, padding=\"same\")\n",
        "        psf_scatter_conv = psf_scatter_conv.squeeze()\n",
        "\n",
        "        b = attenuation_total * psf_scatter_conv    # Eq. (3)\n",
        "\n",
        "        border_convolution = torch.nn.functional.conv2d(input=boundary_map[None, None, :, :], weight=g_kernel, stride=1, padding=\"same\")\n",
        "        border_convolution = border_convolution.squeeze()\n",
        "\n",
        "        r = attenuation_total * reflection_total * refl_map * border_convolution # Eq. (2)\n",
        "\n",
        "        intensity_map = b + r   # Eq. (1)\n",
        "        intensity_map = intensity_map.squeeze()\n",
        "        intensity_map = torch.clamp(intensity_map, 0, 1)\n",
        "\n",
        "        return intensity_map, attenuation_total, reflection_total_plot, scatterers_map, scattering_probability, border_convolution, texture_noise, b, r\n",
        "\n",
        "\n",
        "    def render_rays(self, W, H):\n",
        "        N_rays = W\n",
        "        t_vals = torch.linspace(0., 1., H).to(device=device)   # 0-1 linearly spaced, shape H\n",
        "        z_vals = t_vals.unsqueeze(0).expand(N_rays , -1) * 4\n",
        "\n",
        "        return z_vals\n",
        "\n",
        "    # warp the linear US image to approximate US image from curvilinear US probe\n",
        "    def warp_img(self, inputImage):\n",
        "        resultWidth = 360\n",
        "        resultHeight = 220\n",
        "        centerX = resultWidth / 2\n",
        "        centerY = -120.0\n",
        "        maxAngle =  60.0 / 2 / 180 * pi #rad\n",
        "        minAngle = -maxAngle\n",
        "        minRadius = 140.0\n",
        "        maxRadius = 340.0\n",
        "\n",
        "        h, w = inputImage.squeeze().shape\n",
        "\n",
        "        import torch.nn.functional as F\n",
        "\n",
        "        # Create x and y grids\n",
        "        x = torch.arange(resultWidth).float() - centerX\n",
        "        y = torch.arange(resultHeight).float() - centerY\n",
        "        xx, yy = torch.meshgrid(x, y)\n",
        "\n",
        "        # Calculate angle and radius\n",
        "        angle = torch.atan2(xx, yy)\n",
        "        radius = torch.sqrt(xx ** 2 + yy ** 2)\n",
        "\n",
        "        # Create masks for angle and radius\n",
        "        angle_mask = (angle > minAngle) & (angle < maxAngle)\n",
        "        radius_mask = (radius > minRadius) & (radius < maxRadius)\n",
        "\n",
        "        # Calculate original column and row\n",
        "        origCol = (angle - minAngle) / (maxAngle - minAngle) * w\n",
        "        origRow = (radius - minRadius) / (maxRadius - minRadius) * h\n",
        "\n",
        "        # Reshape input image to be a batch of 1 image\n",
        "        inputImage = inputImage.float().unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        # Scale original column and row to be in the range [-1, 1]\n",
        "        origCol = origCol / (w - 1) * 2 - 1\n",
        "        origRow = origRow / (h - 1) * 2 - 1\n",
        "\n",
        "        # Transpose input image to have channels first\n",
        "        inputImage = inputImage.permute(0, 1, 3, 2)\n",
        "\n",
        "        # Use grid_sample to interpolate\n",
        "        grid = torch.stack([origCol, origRow], dim=-1).unsqueeze(0).to(device)\n",
        "        resultImage = F.grid_sample(inputImage, grid, mode='bilinear', align_corners=True)\n",
        "\n",
        "        # Apply masks and set values outside of mask to 0\n",
        "        resultImage[~(angle_mask.unsqueeze(0).unsqueeze(0) & radius_mask.unsqueeze(0).unsqueeze(0))] = 0.0\n",
        "        resultImage_resized = transforms.Resize((256,256))(resultImage).float().squeeze()\n",
        "\n",
        "        return resultImage_resized\n",
        "\n",
        "\n",
        "    def forward(self, ct_slice):\n",
        "        if self.params[\"debug\"]: self.plot_fig(ct_slice, \"ct_slice\", False)\n",
        "\n",
        "        #init tissue maps\n",
        "        #generate 2D acousttic_imped map\n",
        "        self.acoustic_imped_map = self.map_dict_to_array(self.acoustic_impedance_dict, ct_slice)#.astype('int64'))\n",
        "\n",
        "        #generate 2D attenuation map\n",
        "        self.attenuation_medium_map = self.map_dict_to_array(self.attenuation_dict, ct_slice)\n",
        "\n",
        "        if self.params[\"debug\"]:\n",
        "            self.plot_fig(self.acoustic_imped_map, \"acoustic_imped_map\", False)\n",
        "            self.plot_fig(self.attenuation_medium_map, \"attenuation_medium_map\", False)\n",
        "\n",
        "        self.mu_0_map = self.map_dict_to_array(self.mu_0_dict, ct_slice)\n",
        "\n",
        "        self.mu_1_map = self.map_dict_to_array(self.mu_1_dict, ct_slice)\n",
        "\n",
        "        self.sigma_0_map = self.map_dict_to_array(self.sigma_0_dict, ct_slice)\n",
        "\n",
        "        self.acoustic_imped_map = torch.rot90(self.acoustic_imped_map, 1, [0, 1])\n",
        "        diff_arr = torch.diff(self.acoustic_imped_map, dim=0)\n",
        "\n",
        "        diff_arr = torch.cat((torch.zeros(diff_arr.shape[1], dtype=torch.float32).unsqueeze(0).to(device=device), diff_arr))\n",
        "\n",
        "        boundary_map =  -torch.exp(-(diff_arr**2)/alpha_coeff_boundary_map) + 1\n",
        "\n",
        "        boundary_map = torch.rot90(boundary_map, 3, [0, 1])\n",
        "\n",
        "        if self.params[\"debug\"]:\n",
        "           self.plot_fig(diff_arr, \"diff_arr\", False)\n",
        "           self.plot_fig(boundary_map, \"boundary_map\", True)\n",
        "\n",
        "        shifted_arr = torch.roll(self.acoustic_imped_map, -1, dims=0)\n",
        "        shifted_arr[-1:] = 0\n",
        "\n",
        "        sum_arr = self.acoustic_imped_map + shifted_arr\n",
        "        sum_arr[sum_arr == 0] = 1\n",
        "        div = diff_arr / sum_arr\n",
        "\n",
        "        refl_map = div ** 2\n",
        "        refl_map = torch.sigmoid(refl_map)      # 1 / (1 + (-refl_map).exp())\n",
        "        refl_map = torch.rot90(refl_map, 3, [0, 1])\n",
        "\n",
        "        if self.params[\"debug\"]: self.plot_fig(refl_map, \"refl_map\", True)\n",
        "\n",
        "        z_vals = self.render_rays(ct_slice.shape[0], ct_slice.shape[1])\n",
        "\n",
        "        if CLAMP_VALS:\n",
        "            self.clamp_map_ranges()\n",
        "\n",
        "        ret_list = self.rendering(ct_slice.shape[0], ct_slice.shape[1], z_vals=z_vals, refl_map=refl_map, boundary_map=boundary_map)\n",
        "\n",
        "        intensity_map  = ret_list[0]\n",
        "\n",
        "        if self.params[\"debug\"]:\n",
        "            self.plot_fig(intensity_map, \"intensity_map\", True)\n",
        "\n",
        "            result_list = [\"intensity_map\", \"attenuation_total\", \"reflection_total\",\n",
        "                            \"scatters_map\", \"scattering_probability\", \"border_convolution\",\n",
        "                            \"texture_noise\", \"b\", \"r\"]\n",
        "\n",
        "            for k in range(len(ret_list)):\n",
        "                result_np = ret_list[k]\n",
        "                if torch.is_tensor(result_np):\n",
        "                    result_np = result_np.detach().cpu().numpy()\n",
        "\n",
        "                if k==2:\n",
        "                    self.plot_fig(result_np, result_list[k], False)\n",
        "                else:\n",
        "                    self.plot_fig(result_np, result_list[k], True)\n",
        "                # print(result_list[k], \", \", result_np.shape)\n",
        "\n",
        "        intensity_map_masked = self.warp_img(intensity_map)\n",
        "        intensity_map_masked = torch.rot90(intensity_map_masked, 3)\n",
        "\n",
        "        if self.params[\"debug\"]:  self.plot_fig(intensity_map_masked, \"intensity_map_masked\", True)\n",
        "\n",
        "        return intensity_map_masked\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqNUqXHUcNbN"
      },
      "source": [
        "Segmentation, Composition and US slicing code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "FX05Q45ymtFo"
      },
      "outputs": [],
      "source": [
        "total_lmap = {\"0\": 0, \"1\": 12, \"2\": 6, \"3\": 6, \"4\": 8, \"5\": 11, \"6\": 8, \"7\": 12, \"8\": 12, \"9\": 12, \"10\": 2, \"11\": 2, \"12\": 2, \"13\": 2, \"14\": 2, \"15\": 8, \"16\": 0, \"17\": 0, \"18\": 8, \"19\": 12, \"20\": 8, \"21\": 0, \"22\": 0, \"23\": 6, \"24\": 6, \"25\": 13, \"26\": 13, \"27\": 13, \"28\": 13, \"29\": 13, \"30\": 13, \"31\": 13, \"32\": 13, \"33\": 13, \"34\": 13, \"35\": 13, \"36\": 13, \"37\": 13, \"38\": 13, \"39\": 13, \"40\": 13, \"41\": 13, \"42\": 13, \"43\": 13, \"44\": 13, \"45\": 13, \"46\": 13, \"47\": 13, \"48\": 13, \"49\": 13, \"50\": 13, \"51\": 8, \"52\": 4, \"53\": 4, \"54\": 8, \"55\": 4, \"56\": 4, \"57\": 4, \"58\": 4, \"59\": 4, \"60\": 4, \"61\": 4, \"62\": 4, \"63\": 4, \"64\": 4, \"65\": 4, \"66\": 4, \"67\": 4, \"68\": 4, \"69\": 13, \"70\": 13, \"71\": 13, \"72\": 13, \"73\": 13, \"74\": 13, \"75\": 0, \"76\": 0, \"77\": 0, \"78\": 0, \"79\": 0, \"80\": 0, \"81\": 0, \"82\": 0, \"83\": 0, \"84\": 0, \"85\": 0, \"86\": 0, \"87\": 0, \"88\": 0, \"89\": 0, \"90\": 0, \"91\": 0, \"92\": 13, \"93\": 13, \"94\": 13, \"95\": 13, \"96\": 13, \"97\": 13, \"98\": 13, \"99\": 13, \"100\": 13, \"101\": 13, \"102\": 13, \"103\": 13, \"104\": 13, \"105\": 13, \"106\": 13, \"107\": 13, \"108\": 13, \"109\": 13, \"110\": 13, \"111\": 13, \"112\": 13, \"113\": 13, \"114\": 13, \"115\": 13, \"116\": 13, \"117\": 0}\n",
        "\n",
        "name2label = {\n",
        "            \"total\": {\n",
        "                \"2\": [\"lung\"],\n",
        "                \"4\": [\"aorta\", \"artery\", \"atrial\", \"iliac\", \"vein\",\"vena\"],\n",
        "                \"6\": [\"kidney\"],\n",
        "                \"8\": [\"bowel\", \"colon\", \"esophagus\", \"gallbladder\", \"heart\", \"stomach\", \"trunk\", \"autochlon\", \"iliopsoas\", \"gluteus\"],\n",
        "                \"11\": [\"liver\"],\n",
        "                \"12\": [\"adrenal_gland\", \"duodenum\", \"pancreas\", \"spleen\"],\n",
        "                \"13\": [\"clavicula\", \"humerus\", \"rib_\", \"vertebrae_\", \"sacrum\", \"scapula\", \"sternum\", \"femur\", \"hip\", \"fibula\", \"tibia\", \"radius\", \"ulna\", \"carpal\", \"tarsal\", \"patella\"]\n",
        "            },\n",
        "            \"body\":{\n",
        "                \"bg\": 9,\n",
        "                \"skin\": 12,\n",
        "                \"fat\": 3,\n",
        "                \"muscle\": 8\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "palettedata = [ 0,0,0, 0,0,0, 220,30,30, 170,80,0, 0,170,0, 0,0,0, 0,175,20, 0,0,0, 0,170,190, 0,0,0, 0,0,0, 0,120,230, 115,65,200, 255,0,150]\n",
        "\n",
        "pointpalette = [torch.tensor([[0,0,0, 255],\n",
        "                            [0,0,0, 255],\n",
        "                            [220,30,30, 255],\n",
        "                            [170,80,0, 31],\n",
        "                            [0,170,0, 255],\n",
        "                            [0,0,0, 255],\n",
        "                            [0,175,20, 255],\n",
        "                            [0,0,0, 255],\n",
        "                            [0,170,190, 255],\n",
        "                            [0,0,0, 255],\n",
        "                            [0,0,0, 255],\n",
        "                            [0,120,230, 255],\n",
        "                            [115,65,200, 31],\n",
        "                            [255,0,150, 255]]),\n",
        "                [0, 0, 100000, 100000, 100000, 0, 100000, 0, 100000, 0, 0, 100000, 400000, 100000]]\n",
        "\n",
        "# from torch.nn import OptimizedModule\n",
        "def dict_2_map(d: dict[list[uint8], uint8]) -> list[list[uint8]]:\n",
        "    map = [[] for _ in range(15)]\n",
        "\n",
        "    for k, v in d.items():\n",
        "        int_k = uint8(k)\n",
        "        map[v].append(int_k)\n",
        "\n",
        "    return map\n",
        "\n",
        "def batched(self, iterable, n):\n",
        "    it = iter(iterable)\n",
        "    while batch := tuple(islice(it, n)):\n",
        "        yield batch\n",
        "\n",
        "# Save time by initializing predictors once, instead of for each task\n",
        "def initialize_predictors(device,\n",
        "                        folds: list = (0,)) -> dict:\n",
        "    \"\"\"\n",
        "    Initialize nnUNetPredictor instances for each segmentation task.\n",
        "\n",
        "    Args:\n",
        "        device (str): Device to run predictions on (device, 'cpu', 'mps').\n",
        "        use_folds (tuple): Fold indices to use for prediction.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary mapping task names to their respective nnUNetPredictor instances.\n",
        "    \"\"\"\n",
        "    # Define tasks\n",
        "    tasks = [(\"total\",\n",
        "            [291, 292, 293, 294, 295],\n",
        "            [\"Dataset291_TotalSegmentator_part1_organs_1559subj\",\n",
        "            \"Dataset292_TotalSegmentator_part2_vertebrae_1532subj\",\n",
        "            \"Dataset293_TotalSegmentator_part3_cardiac_1559subj\",\n",
        "            \"Dataset294_TotalSegmentator_part4_muscles_1559subj\",\n",
        "            \"Dataset295_TotalSegmentator_part5_ribs_1559subj\"],\n",
        "            [\"/v2.0.0-weights/Dataset291_TotalSegmentator_part1_organs_1559subj.zip\",\n",
        "            \"/v2.0.0-weights/Dataset292_TotalSegmentator_part2_vertebrae_1532subj.zip\",\n",
        "            \"/v2.0.0-weights/Dataset293_TotalSegmentator_part3_cardiac_1559subj.zip\",\n",
        "            \"/v2.0.0-weights/Dataset294_TotalSegmentator_part4_muscles_1559subj.zip\",\n",
        "            \"/v2.0.0-weights/Dataset295_TotalSegmentator_part5_ribs_1559subj.zip\"],\n",
        "            \"nnUNetTrainerNoMirroring\",\n",
        "            False),\n",
        "            (\"tissue_types\",\n",
        "            [481],\n",
        "            [\"Dataset481_tissue_1559subj\"],\n",
        "            [],\n",
        "            \"nnUNetTrainer\",\n",
        "            True),\n",
        "            (\"body\",\n",
        "            [299],\n",
        "            [\"Dataset299_body_1559subj\"],\n",
        "            [\"/v2.0.0-weights/Dataset299_body_1559subj.zip\"],\n",
        "            \"nnUNetTrainer\",\n",
        "            False)]\n",
        "\n",
        "    commercial_models_inv = {v: k for k, v in commercial_models.items()}\n",
        "    base_url = \"https://github.com/wasserth/TotalSegmentator/releases/download\"\n",
        "\n",
        "    # Get weights directory\n",
        "    weights_dir = get_weights_dir()\n",
        "    os.makedirs(weights_dir, exist_ok=True)\n",
        "\n",
        "    predictors = {}\n",
        "    for task_name, task_ids, paths, urls, trainer,with_license in tasks:\n",
        "        print(f\"INIT: {task_name} predictor\")\n",
        "        if with_license:\n",
        "            for i in range(len(task_ids)):\n",
        "                cfg_dataset = weights_dir / paths[i] / (trainer + '__nnUNetPlans__3d_fullres') / 'dataset.json'\n",
        "                if paths[i] not in os.listdir(weights_dir):\n",
        "                    download_model_with_license_and_unpack(commercial_models_inv[task_ids[i]], weights_dir)\n",
        "\n",
        "                    # # directly remaps label assignments, saving time\n",
        "                    # with open(cfg_dataset, mode='r') as f:\n",
        "                    #     data = json.load(f)\n",
        "\n",
        "                    # data['labels']['subcutaneous_fat'] = name2label['body']['fat']\n",
        "                    # data['labels']['torso_fat'] = name2label['body']['fat']\n",
        "                    # data['labels']['skeletal_muscle'] = name2label['body']['muscle']\n",
        "\n",
        "                    # with open(cfg_dataset, mode='w') as f:\n",
        "                    #     json.dump(data, f, indent=4)\n",
        "\n",
        "\n",
        "                # Initialize the predictor\n",
        "                predictor = nnUNetPredictor(\n",
        "                    tile_step_size=0.5,\n",
        "                    use_gaussian=True,\n",
        "                    use_mirroring=False,\n",
        "                    perform_everything_on_device=(device.type == device),\n",
        "                    device=device,\n",
        "                    verbose=True,\n",
        "                    allow_tqdm=True\n",
        "                )\n",
        "                # Initialize from the trained model folder\n",
        "                predictor.initialize_from_trained_model_folder(\n",
        "                    str(weights_dir / paths[i] / (trainer + \"__nnUNetPlans__3d_fullres\")),\n",
        "                    use_folds=folds,\n",
        "                    checkpoint_name='checkpoint_final.pth'\n",
        "                )\n",
        "\n",
        "                predictors[task_ids[i]] = predictor\n",
        "\n",
        "        else:\n",
        "            for i in range(len(urls)):\n",
        "                cfg_dataset = weights_dir / paths[i] / (trainer + '__nnUNetPlans__3d_fullres') / 'dataset.json'\n",
        "                if paths[i] not in os.listdir(weights_dir):\n",
        "                    download_url_and_unpack(base_url + urls[i], weights_dir)\n",
        "\n",
        "                    # # directly remaps label assignments, saving time\n",
        "                    # with open(cfg_dataset, mode='r') as f:\n",
        "                    #     data = json.load(f)\n",
        "\n",
        "                    # if task_name == 'total':\n",
        "                    #     for name, value in data['labels'].items():\n",
        "                    #         data['labels'][name] = total_lmap[str(value)]\n",
        "                    # else:\n",
        "                    #     # TODO we could merge body trunc with extremeties and get free skin generation for the whole body\n",
        "                    #     pass\n",
        "\n",
        "                    # with open(cfg_dataset, mode='w') as f:\n",
        "                    #     json.dump(data, f, indent=4)\n",
        "\n",
        "\n",
        "                # Initialize the predictor\n",
        "                predictor = nnUNetPredictor(\n",
        "                    tile_step_size=0.5,\n",
        "                    use_gaussian=True,\n",
        "                    # use_mirroring=(task_name!='total'),\n",
        "                    use_mirroring=False,\n",
        "                    perform_everything_on_device=(device.type == device),\n",
        "                    device=device,\n",
        "                    verbose=True,\n",
        "                    allow_tqdm=True\n",
        "                )\n",
        "                # Initialize from the trained model folder\n",
        "                predictor.initialize_from_trained_model_folder(\n",
        "                    str(weights_dir / paths[i] / (trainer + \"__nnUNetPlans__3d_fullres\")),\n",
        "                    use_folds=folds,\n",
        "                    checkpoint_name='checkpoint_final.pth'\n",
        "                )\n",
        "                predictors[task_ids[i]] = predictor\n",
        "\n",
        "    return predictors\n",
        "\n",
        "def bin_erosion(kernel:torch.Tensor, padded:torch.Tensor, ret:torch.Tensor):\n",
        "    # Assumes stacked 3d and no normalization needed\n",
        "    i, hdx, idx, jdx = cuda.grid(4)\n",
        "\n",
        "    # Run kernel\n",
        "    window = padded[i,\n",
        "                    hdx-int((kernel.shape[0]-1) / 2):hdx+int((kernel.shape[0]-1) / 2),\n",
        "                    idx-int((kernel.shape[0]-1) / 2):idx+int((kernel.shape[0]-1) / 2),\n",
        "                    jdx-int((kernel.shape[0]-1) / 2):jdx+int((kernel.shape[0]-1) / 2)]\n",
        "    # TODO: does this also get JITed?\n",
        "    match = torch.all(kernel == window)\n",
        "    ret[i, hdx, idx, jdx] = 1 if match else 0\n",
        "\n",
        "def bin_dilation(kernel:torch.Tensor, padded:torch.Tensor, ret:torch.Tensor):\n",
        "    # Assumes stacked 3d and no normalization needed\n",
        "    i, hdx, idx, jdx = cuda.grid(4)\n",
        "\n",
        "    # Run kernel\n",
        "    window = padded[i,\n",
        "                    hdx-int((kernel.shape[0]-1) / 2):hdx+int((kernel.shape[0]-1) / 2),\n",
        "                    idx-int((kernel.shape[0]-1) / 2):idx+int((kernel.shape[0]-1) / 2),\n",
        "                    jdx-int((kernel.shape[0]-1) / 2):jdx+int((kernel.shape[0]-1) / 2)]\n",
        "    # TODO: does this also get JITed?\n",
        "    match = torch.any(kernel == window)\n",
        "    ret[i, hdx, idx, jdx] = 1 if match else 0\n",
        "\n",
        "class CT2US(torch.nn.Module):\n",
        "    def seg_predictor(self, imgs, properties, task, resamp_thr):\n",
        "        return self.predictors[task].predict_from_list_of_npy_arrays(imgs,\n",
        "                                                    None,\n",
        "                                                    properties,\n",
        "                                                    None, 2, save_probabilities=False,\n",
        "                                                    num_processes_segmentation_export=resamp_thr)\n",
        "\n",
        "    def seg_new(self, imgs, properties, task, resamp_thr):\n",
        "        return self.predictors[task].predict_from_data_iterator(\n",
        "                                        self.iterator(self.predictors[task], imgs, properties),\n",
        "                                        save_probabilities=False,\n",
        "                                        num_processes_segmentation_export=resamp_thr\n",
        "                                    )\n",
        "\n",
        "        # Does not work for some reason, totalsegmentator returns zeros instead of labels\n",
        "    def seg_old(self, imgs, properties, task, resamp_thr):\n",
        "        ret = []\n",
        "\n",
        "        # TODO Get list from \"total\" labels and find matches in totalsegmentator\n",
        "        roi = [l for _, l in self.name2label[\"total\"].items()]\n",
        "        roi = np.concatenate(roi).tolist()\n",
        "        if task == \"total\":\n",
        "            for img in imgs:\n",
        "                ret.append(np.asarray(ts.totalsegmentator(\n",
        "                                    input=img,\n",
        "                                    task=task,\n",
        "                                    nr_thr_resamp=resamp_thr\n",
        "                                    # roi_subset=roi\n",
        "                                ).dataobj, dtype=np.uint8))\n",
        "\n",
        "        else:\n",
        "            for img in imgs:\n",
        "                ret.append(np.asarray(ts.totalsegmentator(\n",
        "                                    input=img,\n",
        "                                    task=task,\n",
        "                                    nr_thr_resamp=resamp_thr\n",
        "                                ).dataobj, dtype=np.uint8))\n",
        "\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def __init__(self, method: str = 'old'):\n",
        "        super(CT2US, self).__init__()\n",
        "        methods = {'old', 'new', 'predictor'}\n",
        "        if not method in methods:\n",
        "            raise KeyError(f\"Method not supported, choose from {methods}\")\n",
        "        else:\n",
        "            self.method = method\n",
        "\n",
        "\n",
        "        self.device = device\n",
        "        if device.type == device and torch.cuda.is_available():\n",
        "            self.m = cp\n",
        "            self.dil_t = cuda.jit(bin_dilation)\n",
        "            self.er_t = cuda.jit(bin_erosion)\n",
        "            self.ops = cusci\n",
        "\n",
        "        else:\n",
        "            self.m = np\n",
        "            self.dil_t = njit(bin_dilation)\n",
        "            self.er_t = njit(bin_erosion)\n",
        "            self.ops = scipy.ndimage\n",
        "\n",
        "        if not method == 'old':\n",
        "            predictors = initialize_predictors(device=device, folds=[0])\n",
        "            self.predictors = predictors\n",
        "            self.predictor_keys = predictors.keys()\n",
        "\n",
        "        segmentator = {\n",
        "            # 'new': self.predict_tensor_iter,\n",
        "            'new': self.seg_new,\n",
        "            'predictor': self.seg_predictor,\n",
        "            'old': self.seg_old\n",
        "        }\n",
        "        self.segmentator = segmentator[method]\n",
        "\n",
        "        us = {\n",
        "            'new': self.to_us_sim_old,\n",
        "            'predictor': self.to_us_sim_old,\n",
        "            'old': self.to_us_sim_old\n",
        "        }\n",
        "        self.us = us[method]\n",
        "\n",
        "        composer = {\n",
        "            'new': self.stacked_assemble,\n",
        "            'predictor': self.stacked_assemble,\n",
        "            'old': self.assemble\n",
        "        }\n",
        "        self.composer = composer[method]\n",
        "\n",
        "        hparams = {\n",
        "            'debug' : False,\n",
        "            'device' : device\n",
        "        }\n",
        "\n",
        "        self.ultrasound_rendering = UltrasoundRendering(hparams, default_param=True)\n",
        "\n",
        "        self.total_lmap = total_lmap\n",
        "        self.name2label = name2label\n",
        "        self.tmap = dict_2_map(self.total_lmap)\n",
        "\n",
        "    def bin_dilation(self, imgs:torch.Tensor, kernel_size:int=3 ,iterations:int=1):\n",
        "        kernel = torch.ones((kernel_size, kernel_size, kernel_size), dtype=torch.uint8)\n",
        "        if imgs.is_cuda:\n",
        "            d_imgs = cuda.as_cuda_array(imgs.detach())\n",
        "            kernel = cuda.as_cuda_array(kernel.detach())\n",
        "            threadsperblock = (1, kernel_size, kernel_size, kernel_size)\n",
        "            blocks = (imgs.shape[0],\n",
        "                        np.ceil(imgs.shape[1] / threadsperblock[1]),\n",
        "                        np.ceil(imgs.shape[2] / threadsperblock[2]),\n",
        "                        np.ceil(imgs.shape[3] / threadsperblock[3]))\n",
        "            for _ in iterations:\n",
        "                ret = cuda.as_cuda_array(torch.zeros(imgs.shape, device=imgs.device).detach())\n",
        "                padded = cuda.as_cuda_array(\n",
        "                            imgs.to_padded_tensor(\n",
        "                                padding=0,\n",
        "                                output_size=(imgs.shape[0], imgs.shape[1] + 2, imgs.shape[2] + 2,imgs.shape[3] + 2)\n",
        "                            ).detach())\n",
        "                self._dil_cuda[blocks, threadsperblock](kernel, padded, ret)\n",
        "                d_imgs = ret\n",
        "        else:\n",
        "            d_imgs = imgs.detach().numpy()\n",
        "            kernel = kernel.detach()\n",
        "            for _ in iterations:\n",
        "                ret = torch.zeros(imgs.shape, device=imgs.device).detach()\n",
        "                padded = imgs.to_padded_tensor(\n",
        "                            padding=0,\n",
        "                            output_size=(imgs.shape[0], imgs.shape[1] + 2, imgs.shape[2] + 2,imgs.shape[3] + 2)\n",
        "                        ).detach()\n",
        "\n",
        "                self._dil_cpu(kernel, padded, ret)\n",
        "                d_imgs = ret\n",
        "\n",
        "        return d_imgs\n",
        "\n",
        "    def bin_erosion(self, imgs:torch.Tensor, kernel_size:int=3 ,iterations:int=1):\n",
        "        kernel = torch.ones((kernel_size, kernel_size, kernel_size), dtype=torch.uint8)\n",
        "        if imgs.is_cuda:\n",
        "            d_imgs = cuda.as_cuda_array(imgs.detach())\n",
        "            kernel = cuda.as_cuda_array(kernel.detach())\n",
        "            threadsperblock = (1, kernel_size, kernel_size, kernel_size)\n",
        "            blocks = (imgs.shape[0],\n",
        "                        np.ceil(imgs.shape[1] / threadsperblock[1]),\n",
        "                        np.ceil(imgs.shape[2] / threadsperblock[2]),\n",
        "                        np.ceil(imgs.shape[3] / threadsperblock[3]))\n",
        "            for _ in iterations:\n",
        "                ret = cuda.as_cuda_array(torch.zeros(imgs.shape, device=imgs.device).detach())\n",
        "                padded = cuda.as_cuda_array(\n",
        "                            imgs.to_padded_tensor(\n",
        "                                padding=0,\n",
        "                                output_size=(imgs.shape[0], imgs.shape[1] + 2, imgs.shape[2] + 2,imgs.shape[3] + 2)\n",
        "                            ).detach())\n",
        "                self._er_cuda[blocks, threadsperblock](kernel, padded, ret)\n",
        "                d_imgs = ret\n",
        "        else:\n",
        "            d_imgs = imgs.detach().numpy()\n",
        "            kernel = kernel.detach()\n",
        "            for _ in iterations:\n",
        "                ret = torch.zeros(imgs.shape, device=imgs.device).detach()\n",
        "                padded = imgs.to_padded_tensor(\n",
        "                            padding=0,\n",
        "                            output_size=(imgs.shape[0], imgs.shape[1] + 2, imgs.shape[2] + 2,imgs.shape[3] + 2)\n",
        "                        ).detach()\n",
        "\n",
        "                self._er_cpu(kernel, padded, ret)\n",
        "                d_imgs = ret\n",
        "\n",
        "        return d_imgs\n",
        "\n",
        "    # Adapted from nnUNetPredictor\n",
        "    def iterator(self,\n",
        "                predictor: nnUNetPredictor,\n",
        "                imgs: list[np.ndarray],\n",
        "                properties: list[dict]):\n",
        "\n",
        "        # MAYBE: look at data_iterators.preprocess_fromnpy_save_to_queue for vstack use for ROI foreground masking\n",
        "\n",
        "        # pp = predictor.get_data_iterator_from_raw_npy_data(\n",
        "        #     imgs,\n",
        "        #     properties\n",
        "        # )\n",
        "\n",
        "        # preprocessor = predictor.configuration_manager.preprocessor_class(verbose=predictor.verbose)\n",
        "\n",
        "        # properties = {key: [i[key] for i in properties] for key in properties[0]}\n",
        "\n",
        "        # data, seg = preprocessor.run_case_npy(\n",
        "        #                 np.stack(imgs),\n",
        "        #                 None,\n",
        "        #                 properties,\n",
        "        #                 predictor.plans_manager,\n",
        "        #                 predictor.configuration_manager,\n",
        "        #                 predictor.dataset_json\n",
        "        #             )\n",
        "\n",
        "        # pass\n",
        "\n",
        "        preprocessor = predictor.configuration_manager.preprocessor_class(verbose=predictor.verbose)\n",
        "        for a, p in zip(imgs, properties):\n",
        "            data, seg = preprocessor.run_case_npy(a,\n",
        "                                                  None,\n",
        "                                                  p,\n",
        "                                                  predictor.plans_manager,\n",
        "                                                  predictor.configuration_manager,\n",
        "                                                  predictor.dataset_json)\n",
        "            yield {'data': torch.from_numpy(data).contiguous().pin_memory(), 'data_properties': p, 'ofile': None}\n",
        "\n",
        "    def convert_logits_to_segmentation(self, prediction, properties, predictor):\n",
        "        spacing_transposed = [properties['spacing'][i] for i in predictor.plans_manager.transpose_forward]\n",
        "        current_spacing = predictor.configuration_manager.spacing if \\\n",
        "            len(predictor.configuration_manager.spacing) == \\\n",
        "            len(properties['shape_after_cropping_and_before_resampling']) else \\\n",
        "            [spacing_transposed[0], *predictor.configuration_manager.spacing]\n",
        "        predicted_logits = predictor.configuration_manager.resampling_fn_probabilities(predicted_logits,\n",
        "                                                properties['shape_after_cropping_and_before_resampling'],\n",
        "                                                current_spacing,\n",
        "                                                [properties['spacing'][i] for i in predictor.plans_manager.transpose_forward])\n",
        "        # return value of resampling_fn_probabilities can be ndarray or Tensor but that does not matter because\n",
        "        # apply_inference_nonlin will convert to torch\n",
        "        predicted_probabilities = predictor.label_manager.apply_inference_nonlin(predicted_logits)\n",
        "        del predicted_logits\n",
        "        segmentation = predictor.label_manager.convert_probabilities_to_segmentation(predicted_probabilities)\n",
        "\n",
        "        # put segmentation in bbox (revert cropping)\n",
        "        segmentation_reverted_cropping = np.zeros(properties['shape_before_cropping'],\n",
        "                                                dtype=np.uint8 if len(predictor.label_manager.foreground_labels) < 255 else np.uint16)\n",
        "        slicer = tuple([slice(*i) for i in properties['bbox_used_for_cropping']])\n",
        "        segmentation_reverted_cropping[slicer] = segmentation\n",
        "        del segmentation\n",
        "\n",
        "        pass\n",
        "\n",
        "    # Adapted from nnUNetPredictor\n",
        "    def predict_tensor_iter(self,\n",
        "                        data_iterator) -> list[torch.tensor]:\n",
        "\n",
        "        r = []\n",
        "        for preprocessed in data_iterator:\n",
        "            asm = []\n",
        "            data = preprocessed['data']\n",
        "\n",
        "            properties = preprocessed['data_properties']\n",
        "\n",
        "            for predictor in self.predictors.values():\n",
        "                old_threads = torch.get_num_threads()\n",
        "                # HYPERPARAMETER: number of threads to use for prediction\n",
        "                default_num_processes = 4\n",
        "                torch.set_num_threads(default_num_processes if default_num_processes < old_threads else old_threads)\n",
        "                prediction = None\n",
        "\n",
        "                for params in predictor.list_of_parameters:\n",
        "\n",
        "                    # messing with state dict names...\n",
        "                    # if not isinstance(predictor.network, OptimizedModule):\n",
        "                    #     predictor.network.load_state_dict(params)\n",
        "                    # else:\n",
        "                    #     predictor.network._orig_mod.load_state_dict(params)\n",
        "\n",
        "                    if prediction is None:\n",
        "                        prediction = predictor.predict_sliding_window_return_logits(data)\n",
        "                    else:\n",
        "                        prediction += predictor.predict_sliding_window_return_logits(data)\n",
        "\n",
        "                if len(predictor.list_of_parameters) > 1:\n",
        "                    prediction /= len(self.list_of_parameters)\n",
        "\n",
        "                prediction = self.convert_logits_to_segmentation(prediction, properties, predictor)\n",
        "\n",
        "            print(f'\\nDone with image of shape {data.shape}:')\n",
        "\n",
        "            # clear lru cache\n",
        "            compute_gaussian.cache_clear()\n",
        "            # clear device cache\n",
        "            if device.type == device:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            r.append()\n",
        "\n",
        "        return [i.get()[0] for i in r]\n",
        "\n",
        "    def assemble(self,\n",
        "                task:str,\n",
        "                segs:list[np.ndarray],\n",
        "                bases:list[np.ndarray],\n",
        "                prev:list[np.ndarray]) -> list[np.ndarray]:\n",
        "\n",
        "        print(f\"ASSEMBLY STARTED: {task}\")\n",
        "        # Process total segmentation\n",
        "\n",
        "        if task == 'total':\n",
        "            for j in range(len(segs)):\n",
        "                for i in range(len(self.tmap)):\n",
        "                    if len(self.tmap[i]) > 0:  # if there are any keys for this value\n",
        "                        a = self.m.where(self.m.isin(self.m.asarray(segs[j], dtype=self.m.uint8), self.m.array(self.tmap[i])), self.m.uint8(i), self.m.uint8(0))\n",
        "                        prev[j] += a\n",
        "\n",
        "        if task == 'tissue_types':\n",
        "            for j in range(len(segs)):\n",
        "                t = self.m.asarray(segs[j])\n",
        "\n",
        "                prev[j][t == 1] = self.m.uint8(self.name2label[\"body\"][\"fat\"])\n",
        "                prev[j][t == 2] = self.m.uint8(self.name2label[\"body\"][\"fat\"])\n",
        "\n",
        "        if task == 'body':\n",
        "            for j in range(len(segs)):\n",
        "                t = self.m.asarray(segs[j])\n",
        "\n",
        "                body = self.ops.binary_dilation(t == 1, iterations=1).astype(self.m.uint8)\n",
        "                body_inner = self.ops.binary_erosion(t, iterations=3, brute_force=True).astype(self.m.uint8)\n",
        "\n",
        "                skin = body - body_inner\n",
        "\n",
        "                # Segment by density\n",
        "                # Roughly the skin density range. Made large to make segmentation not have holes\n",
        "                # (0 to 250 would have many small holes in skin)\n",
        "                density_mask = (bases[j] > -200) & (bases[j] < 250)\n",
        "                skin[~density_mask] = 0\n",
        "\n",
        "                # Fill holes\n",
        "                # skin = binary_closing(skin, iterations=1)  # no real difference\n",
        "                # skin = binary_dilation(skin, iterations=1)  # not good\n",
        "\n",
        "                mask, _ = self.ops.label(skin)\n",
        "                counts = self.m.bincount(mask.flatten())  # number of pixels in each blob\n",
        "\n",
        "                # If only one blob (only background) abort because nothing to remove\n",
        "                if len(counts) > 1:\n",
        "                    remove = self.m.where((counts <= 10) | (counts > 30), True, False)\n",
        "                    remove_idx = self.m.nonzero(remove)[0]\n",
        "                    mask[self.m.isin(self.m.array(mask), remove_idx)] = 0\n",
        "                    mask[mask > 0] = 1\n",
        "\n",
        "                # Removing blobs\n",
        "                # End of snippet from totalsegmentator\n",
        "\n",
        "                dilation_kernel = self.m.ones(shape=(2, 2, 2))\n",
        "\n",
        "                skin = self.m.where(self.ops.binary_dilation(skin == 1, structure=dilation_kernel), self.m.uint8(1), self.m.uint8(0))\n",
        "\n",
        "                prev[j][skin == 1] = self.m.uint8(self.name2label[\"body\"][\"skin\"])\n",
        "\n",
        "                tmp = prev[j].copy()\n",
        "                prev[j][tmp == 0] = self.m.uint8(self.name2label[\"body\"][\"bg\"])\n",
        "\n",
        "        print(\"ASSEMBLY COMPLETED\")\n",
        "\n",
        "        del segs, bases\n",
        "\n",
        "        return prev\n",
        "\n",
        "    def stacked_assemble(self, task:str,\n",
        "                segs:list[np.ndarray],\n",
        "                stacked_bases:list[np.ndarray],\n",
        "                prev: list[np.ndarray]) -> list[np.ndarray]:\n",
        "\n",
        "        print(\"ASSEMBLY STARTED\")\n",
        "\n",
        "        # Process total segmentation\n",
        "        labels = prev\n",
        "\n",
        "        if task == \"total\":\n",
        "            stacked_totals = torch.stack([torch.as_tensor(segs[i], device=self.device) for i in range(stacked_bases.shape[0])], axis=0)\n",
        "            labels[prev != 0] = stacked_totals\n",
        "\n",
        "        elif task == \"tissue_types\":\n",
        "            stacked_tissues = torch.stack([torch.as_tensor(segs[i], device=self.device) for i in range(stacked_bases.shape[0])], axis=0)\n",
        "            labels[stacked_tissues != 0] = stacked_tissues\n",
        "\n",
        "        elif task == \"body\":\n",
        "            stacked_outers = torch.stack([torch.as_tensor(segs[i], device=self.device, dtype=torch.uint8) for i in range(stacked_bases.shape[0])], axis=0)\n",
        "\n",
        "            # Adapted code snippet from totalsegmentator\n",
        "            body = self.bin_dilation(stacked_outers == 1, kernel_size=3, iterations=1).astype(torch.uint8)\n",
        "            body_inner = self.bin_erosion(stacked_outers == 1, kernel_size=3, iterations=3).astype(torch.uint8)\n",
        "            skin = body - body_inner\n",
        "\n",
        "            # Segment by density\n",
        "            # Roughly the skin density range. Made large to make segmentation not have holes\n",
        "            # (0 to 250 would have many small holes in skin)\n",
        "            density_mask = (stacked_bases > -200) & (stacked_bases < 250)\n",
        "            skin[~density_mask] = 0\n",
        "\n",
        "            # Fill holes\n",
        "            # skin = binary_closing(skin, iterations=1)  # no real difference\n",
        "            # skin = binary_dilation(skin, iterations=1)  # not good\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                mask, _ = cusci.label(skin)\n",
        "            else:\n",
        "                mask, _ = scipy.ndimage.label(skin)\n",
        "\n",
        "            counts = torch.bincount(mask.flatten())  # number of pixels in each blob\n",
        "\n",
        "            # If only one blob (only background) abort because nothing to remove\n",
        "            if len(counts) > 1:\n",
        "                remove = torch.where((counts <= 10) | (counts > 30), True, False)\n",
        "                remove_idx = torch.nonzero(remove)[0]\n",
        "                mask[torch.isin(mask, remove_idx)] = 0\n",
        "                mask[mask > 0] = 1\n",
        "\n",
        "            # Removing blobs\n",
        "            # End of snippet from totalsegmentator\n",
        "            mask = torch.where(self.bin_dilation(mask == 1, kernel_size=3, iterations = 2), np.uint8(1), np.uint8(0))\n",
        "\n",
        "            labels[mask == 1] = np.uint8(self.name2label[\"body\"][\"skin\"])\n",
        "\n",
        "            tmp = labels.copy()\n",
        "            labels[tmp == 0] = np.uint8(self.name2label[\"body\"][\"bg\"])\n",
        "\n",
        "        print(\"ASSEMBLY COMPLETED\")\n",
        "        del segs, bases\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def to_us_sim_new(self, segs:list[np.ndarray], properties:dict, dest_us: list[str], step_size:int) -> list[list[np.ndarray]]:\n",
        "        print(\"US SIMULATION STARTED\")\n",
        "\n",
        "        hparams = {\n",
        "            'debug' : False,\n",
        "            'device' : device\n",
        "        }\n",
        "\n",
        "        us_r = UltrasoundRendering(params=hparams, default_param=True).to(hparams['device'])\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Resize([380, 380], transforms.InterpolationMode.NEAREST),\n",
        "                    transforms.CenterCrop((256)),\n",
        "                ])\n",
        "\n",
        "        results = []\n",
        "        for i in range(len(segs)):\n",
        "            us_images = []\n",
        "            labelmap = segs[i].get()\n",
        "            dest = pthlib(dest_us[i]).joinpath(\"slice_\")\n",
        "            os.makedirs(dest.parent, exist_ok=True)\n",
        "\n",
        "            for slice_idx in range(0, labelmap.shape[2], step_size):\n",
        "                slice_data = labelmap[:, :, slice_idx].astype('int64')\n",
        "                labelmap_slice = transform(slice_data).squeeze()\n",
        "\n",
        "                us_image = us_r(labelmap_slice)\n",
        "                us_images.append(us_image.cpu().numpy())\n",
        "\n",
        "                us_image_pil = transforms.ToPILImage()(us_image.cpu().squeeze())\n",
        "                us_image_pil.save(f\"{dest}_{slice_idx}.png\")\n",
        "\n",
        "            results.append(us_images)\n",
        "\n",
        "        print(\"US SIMULATION COMPLETED\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def to_us_sim_old(self, segs:list[np.ndarray], properties:dict, dest_us: list[str],  step_size:int) -> list[tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
        "        print(\"US SIMULATION STARTED\")\n",
        "\n",
        "        hparams = {\n",
        "            'debug' : False,\n",
        "            'device' : self.device\n",
        "        }\n",
        "\n",
        "        l_dict = {\n",
        "            2:'lung',\n",
        "            3:'fat',\n",
        "            4:'vessel',\n",
        "            6:'kidney',\n",
        "            8:'muscle',\n",
        "            11:'liver',\n",
        "            12:'soft tissue',\n",
        "            13:'bone'\n",
        "        }\n",
        "        us_r = UltrasoundRendering(params=hparams, default_param=True).to(device)\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Resize([380, 380], transforms.InterpolationMode.NEAREST),\n",
        "                    transforms.CenterCrop((256)),\n",
        "                ])\n",
        "\n",
        "        us_imgs = []\n",
        "        pcd_base = []\n",
        "\n",
        "        for i in tqdm.tqdm(range(len(segs)), desc=\"Rendering\"):\n",
        "            # p = properties[i][\"spacing\"]\n",
        "            warped = []\n",
        "            us_slices = []\n",
        "            if self.m == cp:\n",
        "                labelmap = segs[i].get()\n",
        "            else:\n",
        "                labelmap = segs[i]\n",
        "\n",
        "            dest = pthlib(dest_us[i]).joinpath(\"slice_\")\n",
        "            os.makedirs(dest.parent, exist_ok=True)\n",
        "\n",
        "            for slice_idx in tqdm.tqdm(range(0, labelmap.shape[2], step_size), desc=\"US slice rendering\"):\n",
        "                slice_data = labelmap[:, :, slice_idx].astype('int64')\n",
        "                labelmap_slice = transform(slice_data).squeeze()\n",
        "\n",
        "                us_slice = us_r(labelmap_slice)\n",
        "                if self.m == np:\n",
        "                    us_slices.append(us_slice.cpu().numpy())\n",
        "                else:\n",
        "                    us_slices.append(us_slice)\n",
        "\n",
        "                us_image_pil = transforms.ToPILImage()(us_slice.cpu().squeeze())\n",
        "                us_image_pil.save(f\"{dest}_{slice_idx}.png\")\n",
        "\n",
        "                # Warp slice to match US and create individual label maps\n",
        "\n",
        "                if self.m == np:\n",
        "                    temp = us_r.warp_img(labelmap_slice.cuda(self.device)).cpu().numpy()\n",
        "                else:\n",
        "                    temp = self.m.asarray(us_r.warp_img(labelmap_slice.cuda(self.device)))\n",
        "\n",
        "                a = self.m.fliplr(self.m.asarray(temp.copy()).transpose(1, 0))\n",
        "\n",
        "                warped.append([(\n",
        "                                    self.ops.binary_fill_holes(\n",
        "                                        # scipy.ndimage.binary_erosion(\n",
        "                                            self.ops.binary_dilation(\n",
        "                                                self.ops.binary_closing(a==tag, iterations=3), iterations=2)),\n",
        "                                                # scipy.ndimage.binary_closing(a==tag, iterations=3), iterations=2), iterations=1)),\n",
        "                                    # scipy.ndimage.binary_erosion(scipy.ndimage.binary_dilation(a==tag, iterations=2), iterations=2),\n",
        "                                    l_dict[tag]\n",
        "                                ) for tag in [2, 3, 4, 6, 8, 11, 12, 13]])\n",
        "\n",
        "                del temp, a\n",
        "\n",
        "            torch.set_default_device(device)\n",
        "            temp = torch.as_tensor(labelmap)\n",
        "\n",
        "            list_pos = []\n",
        "            list_color = []\n",
        "            for i in tqdm.tqdm(range(len(pointpalette[1])), desc=\"Pointcloud rendering\"):\n",
        "                if pointpalette[1][i] != 0:\n",
        "                    base = torch.zeros_like(temp)\n",
        "                    base[temp == i] = 1\n",
        "                    tmp = base.nonzero()\n",
        "\n",
        "                    idx_list = list(range(tmp.shape[0]))\n",
        "                    select_idx = self.m.random.choice(idx_list, size=pointpalette[1][i])\n",
        "                    list_pos.append(tmp[select_idx])\n",
        "\n",
        "                    tmp = torch.zeros((pointpalette[1][i], 4), dtype=torch.uint8)\n",
        "                    tmp[:,...] = torch.as_tensor(pointpalette[0][i])\n",
        "                    list_color.append(tmp)\n",
        "\n",
        "            pcd_base.append([list_pos, list_color, temp.shape])\n",
        "\n",
        "            us_imgs.append(us_slices)\n",
        "            # warped_labels.append(warped)\n",
        "\n",
        "\n",
        "        print(\"US SIMULATION COMPLETED\")\n",
        "\n",
        "        return us_imgs, warped, pcd_base\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                    imgs: list[nifti1.Nifti1Image|np.ndarray],\n",
        "                    properties:list[dict],\n",
        "                    dest_label: list[str],\n",
        "                    dest_us: list[str],\n",
        "                    step_size: int,\n",
        "                    save_labels: bool,\n",
        "                ) -> list[list[np.ndarray]]:\n",
        "\n",
        "        if not self.method == 'old':\n",
        "            bases = torch.stack([\n",
        "                        torch.as_tensor(\n",
        "                            img,\n",
        "                            dtype=torch.float32,\n",
        "                            device=self.device\n",
        "                        ).squeeze() for img in imgs]\n",
        "                    ).cuda(self.device)\n",
        "\n",
        "            f_labels = torch.stack([\n",
        "                            torch.zeros(\n",
        "                                bases[i].shape,\n",
        "                                dtype=torch.uint8,\n",
        "                                device=self.device\n",
        "                            ) for i in range(bases.shape[0])],\n",
        "                            axis=0\n",
        "                        ).cuda(self.device)\n",
        "        else:\n",
        "            bases = [self.m.array(img.dataobj, dtype=self.m.float32) for img in imgs]\n",
        "            f_labels = [self.m.zeros(bases[idx].shape, dtype=self.m.uint8) for idx in range(len(imgs))]\n",
        "\n",
        "        if self.method == \"old\":\n",
        "            tasks = [\"total\", \"tissue_types\", \"body\"]\n",
        "        else:\n",
        "            tasks = list(self.predictor_keys)\n",
        "\n",
        "        print(\"SEGMENTATING:\")\n",
        "\n",
        "        tmp = []\n",
        "        for idx in tqdm.tqdm(range(len(tasks)), desc=\"Segmentating batch\"):\n",
        "\n",
        "            tmp.append(self.segmentator(imgs, properties, tasks[idx], 4))\n",
        "            print(\"SEG DONE!\")\n",
        "\n",
        "        if self.method == 'old':\n",
        "            for idx in tqdm.tqdm(range(len(tasks)), desc=\"Composing into suitable intermediate\"):\n",
        "                f_labels = self.composer(tasks[idx], tmp[idx], bases, f_labels)\n",
        "\n",
        "        us_imgs, warped_labels, pcdb = self.us(f_labels.copy(), properties, dest_us, step_size)\n",
        "\n",
        "        if save_labels:\n",
        "            for idx in tqdm.tqdm(range(len(f_labels)), desc=\"Saving labels\"):\n",
        "                if self.m == cp:\n",
        "                    SimpleITKIO().write_seg(f_labels[idx].get().transpose(2, 1, 0), dest_label[idx], properties[idx])\n",
        "                else:\n",
        "                    SimpleITKIO().write_seg(f_labels[idx].transpose(2, 1, 0), dest_label[idx], properties[idx])\n",
        "\n",
        "                print(f\"SAVED TO '{dest_label[idx]}'\")\n",
        "\n",
        "        t = []\n",
        "        for f in tqdm.tqdm(f_labels, desc=\"Generating warped labels for annotated view\"):\n",
        "            temp = []\n",
        "            if self.m == cp:\n",
        "                labels = f.get()\n",
        "            else:\n",
        "                labels = f\n",
        "\n",
        "            for arr in list(np.flip(labels.copy().transpose(2, 1, 0), 2))[::step_size]:\n",
        "                img = Image.fromarray(arr)\n",
        "                img.putpalette(palettedata *16)\n",
        "                temp.append(img)\n",
        "            t.append(temp)\n",
        "\n",
        "        return [str(pthlib(d).name) for d in dest_label], us_imgs, warped_labels, t, pcdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaDtXb_jcNbP"
      },
      "source": [
        "Dataset loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "BOiTb1SiK5Lk"
      },
      "outputs": [],
      "source": [
        "def collate_tensor(data):\n",
        "    imgs, properties, dest_labels, dest_us = zip(*data)\n",
        "\n",
        "    return torch.from_numpy(imgs), properties, dest_labels, dest_us\n",
        "\n",
        "def collate_list(data):\n",
        "    imgs, properties, dest_labels, dest_us = zip(*data)\n",
        "    return imgs, properties, dest_labels, dest_us\n",
        "\n",
        "# import pandas as pd\n",
        "class CTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir:str, method:str=\"old\", annotations_file:str=None, resample:float=1.5):\n",
        "        self.device = device\n",
        "        self.method = method\n",
        "        if device.type == device:\n",
        "            self.m = cp\n",
        "\n",
        "        else:\n",
        "            self.m = np\n",
        "\n",
        "        self.collate_fn = collate_list\n",
        "\n",
        "        if not isinstance(img_dir, pthlib):\n",
        "            img_dir = pthlib(img_dir)\n",
        "\n",
        "        self.resample = resample\n",
        "\n",
        "        self.img_dir = img_dir\n",
        "        self.annotations_file = annotations_file\n",
        "\n",
        "        l = {\"predictor\" : self.load_bases, \"new\" : self.load_bases, \"old\": self.load_bases}\n",
        "        self.load = l[method]\n",
        "\n",
        "        # r = {\"predictor\" : self.resampler_old, \"new\" : self.resampler_new, \"old\": None}\n",
        "        r = {\"predictor\" : None, \"new\" : None, \"old\": None}\n",
        "        self.resampler = r[method]\n",
        "\n",
        "        if self.annotations_file is not None:\n",
        "            # TODO IMPLEMENT LOADING FROM CSV\n",
        "            # self.img_paths = pd.read_csv(annotations_file)\n",
        "            pass\n",
        "        else:\n",
        "            self.img_paths = glob.glob(f\"{str(self.img_dir)}/*.nii.gz\")\n",
        "            self.img_paths = [(pth,\n",
        "                                pth.replace(\".nii.gz\", \"_label.nii.gz\").replace(\"/imgs/\", \"/labels/\"),\n",
        "                                pth.replace(\".nii.gz\", \"_us\").replace(\"/imgs/\", \"/us/\")\n",
        "                                ) for pth in self.img_paths]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def load_bases(self, pths: str) -> tuple[np.ndarray, dict]:\n",
        "        img, prop = SimpleITKIO().read_images(image_fnames=[join(pths)])\n",
        "        return (img, prop)\n",
        "\n",
        "    def resampler_new(self, img) -> np.ndarray:\n",
        "        spacing = np.array(img[1][\"spacing\"])\n",
        "\n",
        "        new_spacing = np.array(self.resample)\n",
        "\n",
        "        if self.m.array_equal(spacing, new_spacing):\n",
        "            resampled_img = (img)\n",
        "        else:\n",
        "            zoom = spacing / new_spacing\n",
        "\n",
        "        data = self.m.array(img[0], self.m.float32)\n",
        "\n",
        "        new_shape = (np.array(data.shape) * zoom).round().astype(self.m.int32)\n",
        "\n",
        "        if self.device.type == device:\n",
        "            resampled_img = (self.m.array(resize(data, output_shape=new_shape, order=3, mode=\"edge\", anti_aliasing=False)), img[1], img[2], img[3])\n",
        "        else:\n",
        "            resampled_img = (self.m.array(rs.resample_img(data, zoom)), img[1], img[2], img[3])\n",
        "\n",
        "        return resampled_img\n",
        "\n",
        "    def resampler_old(self, img) -> np.ndarray:\n",
        "        spacing = np.array(img[0].header.get_zooms())\n",
        "        new_spacing = np.array(self.resample)\n",
        "\n",
        "        if self.m.array_equal(spacing, new_spacing):\n",
        "            resampled_img = (img)\n",
        "        else:\n",
        "            zoom = spacing / new_spacing\n",
        "\n",
        "        data = self.m.array(img[0].get_fdata(), self.m.float32)\n",
        "\n",
        "        new_shape = (np.array(data.shape) * zoom).round().astype(self.m.int32)\n",
        "\n",
        "        if self.device.type == device:\n",
        "            resampled_img = (self.m.array(resize(data, output_shape=new_shape, order=3, mode=\"edge\", anti_aliasing=False)), {\"spacing\": spacing}, img[1], img[2])\n",
        "        else:\n",
        "            resampled_img = (self.m.array(rs.resample_img(data, zoom)), {\"spacing\": spacing}, img[1], img[2])\n",
        "\n",
        "        return resampled_img\n",
        "\n",
        "    # TODO CLOSEST CANONICAL through transforms?\n",
        "    def __getitem__(self, idx): # -> list[tuple[np.ndarray, dict, str, str]]:\n",
        "        todo = self.img_paths[idx]\n",
        "        img, prop = self.load(todo[0])\n",
        "        dest_label = todo[1]\n",
        "        dest_us = todo[2]\n",
        "\n",
        "        # nnunet predictors require shape (1, x, y, z)\n",
        "        if self.method == \"predictor\":\n",
        "            ret = (img[0][None,...], prop, dest_label, dest_us)\n",
        "        elif self.method == \"new\":\n",
        "            ret = (img, prop, dest_label, dest_us)\n",
        "        elif self.method == \"old\":\n",
        "            affine = np.eye(4)\n",
        "            affine[:3, :3] = np.array(prop[\"sitk_stuff\"][\"direction\"]).reshape(3, 3) * prop[\"sitk_stuff\"][\"spacing\"]\n",
        "            affine[:3, 3] = prop[\"sitk_stuff\"][\"origin\"]\n",
        "\n",
        "            ret = (nifti1.Nifti1Image(img[0].transpose(2, 1, 0), affine=affine), prop, dest_label, dest_us)\n",
        "\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mWpZV-caaT3"
      },
      "source": [
        "# Acquire samples for demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-8Ddh3PbEJE",
        "outputId": "46e3d150-fbb3-4ed1-8e22-2b4e503d25da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading sample data\n",
            "--2025-02-11 00:44:25--  https://www.dropbox.com/scl/fi/y44t2wu7eyg0t3fpknoxv/img.zip?rlkey=5uza6964xrrtffzfc7w977m3z&st=q6u2hzkh&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.85.18, 2620:100:6035:18::a27d:5512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.85.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc525aef5c7596fe071ef5ff42df.dl.dropboxusercontent.com/cd/0/inline/Cj4fSKZSX_yGeuz30jrmXFW0nmQwFCHAXJXWDHMQfPH6ULV0GzcrCITnAXSoN69ZtOO7kvUmPq1lQoFi2QBf0xzo5Nq6m6RJA35s_oLeBkNeq97mSMiQN3-2K_TwTHd52O1Fj39f5Cfi0lwHRCpAzvo3/file?dl=1# [following]\n",
            "--2025-02-11 00:44:26--  https://uc525aef5c7596fe071ef5ff42df.dl.dropboxusercontent.com/cd/0/inline/Cj4fSKZSX_yGeuz30jrmXFW0nmQwFCHAXJXWDHMQfPH6ULV0GzcrCITnAXSoN69ZtOO7kvUmPq1lQoFi2QBf0xzo5Nq6m6RJA35s_oLeBkNeq97mSMiQN3-2K_TwTHd52O1Fj39f5Cfi0lwHRCpAzvo3/file?dl=1\n",
            "Resolving uc525aef5c7596fe071ef5ff42df.dl.dropboxusercontent.com (uc525aef5c7596fe071ef5ff42df.dl.dropboxusercontent.com)... 162.125.85.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc525aef5c7596fe071ef5ff42df.dl.dropboxusercontent.com (uc525aef5c7596fe071ef5ff42df.dl.dropboxusercontent.com)|162.125.85.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Cj6jeHiZE2_ZeuC6eWDaRqEdi7tJy3AOGmLUVFyqF6a4zPD5SxyxFBmegoP9U4ncX5U5o1ka1V8qD_-vIImcXZ8hOFh13czoobjpCCILB4Abmoomy7DYpTKh71ecWJPTYq1qPotl2e24pJ8MKFVt0O9GZZRDSRfC9Jj-ntDR1k_R2cuE1dsGiuoMZTe1_acEgNKaDzPg98DlwMlAfDEpr2FpYMHCAkRAVkCZgVZqjjTABMRtOgjW9zlnWZZ-ojLHXOkurXJxbFub5b9ehD9Ms4BhhDHcuQlq_S02Hw5w9ZC_bWK8xZSsSDAsgLoFNEs1wUiWHJpvW3GH06S1BsbayqB_gCyrlxWLeLSWuDpqCK8sX8Q-_entqmq8m4veMIjpq7U/file?dl=1 [following]\n",
            "--2025-02-11 00:44:27--  https://uc525aef5c7596fe071ef5ff42df.dl.dropboxusercontent.com/cd/0/inline2/Cj6jeHiZE2_ZeuC6eWDaRqEdi7tJy3AOGmLUVFyqF6a4zPD5SxyxFBmegoP9U4ncX5U5o1ka1V8qD_-vIImcXZ8hOFh13czoobjpCCILB4Abmoomy7DYpTKh71ecWJPTYq1qPotl2e24pJ8MKFVt0O9GZZRDSRfC9Jj-ntDR1k_R2cuE1dsGiuoMZTe1_acEgNKaDzPg98DlwMlAfDEpr2FpYMHCAkRAVkCZgVZqjjTABMRtOgjW9zlnWZZ-ojLHXOkurXJxbFub5b9ehD9Ms4BhhDHcuQlq_S02Hw5w9ZC_bWK8xZSsSDAsgLoFNEs1wUiWHJpvW3GH06S1BsbayqB_gCyrlxWLeLSWuDpqCK8sX8Q-_entqmq8m4veMIjpq7U/file?dl=1\n",
            "Reusing existing connection to uc525aef5c7596fe071ef5ff42df.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 135656716 (129M) [application/binary]\n",
            "Saving to: ‘/CT2US/sample/sample.zip’\n",
            "\n",
            "/CT2US/sample/sampl 100%[===================>] 129.37M  27.3MB/s    in 4.9s    \n",
            "\n",
            "2025-02-11 00:44:33 (26.6 MB/s) - ‘/CT2US/sample/sample.zip’ saved [135656716/135656716]\n",
            "\n",
            "Archive:  /CT2US/sample/sample.zip\n",
            "  inflating: /CT2US/sample/img0001.nii.gz  \n",
            "  inflating: /CT2US/sample/img0002.nii.gz  \n",
            "  inflating: /CT2US/sample/img0003.nii.gz  \n"
          ]
        }
      ],
      "source": [
        "todo_dir = pthlib.joinpath(this_folder, \"sample\")\n",
        "todo_dir.mkdir(exist_ok=True)\n",
        "\n",
        "if os.listdir(todo_dir) == []:\n",
        "    print(\"Downloading sample data\")\n",
        "    !wget -O /CT2US/sample/sample.zip \"https://www.dropbox.com/scl/fi/y44t2wu7eyg0t3fpknoxv/img.zip?rlkey=5uza6964xrrtffzfc7w977m3z&st=q6u2hzkh&dl=1\"\n",
        "    !unzip '/CT2US/sample/sample.zip' -d '/CT2US/sample'\n",
        "    !rm '/CT2US/sample/sample.zip'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHLuxFJFcNbR"
      },
      "source": [
        "# Run this for the actual UI\n",
        "\n",
        "- Click the link provided by gradio after running this cell to properly interact with the UI. Debug information can be accessed by simply reading output that is printed out here, even if while using the link.\n",
        "\n",
        "- There is still some work to be done here, especially a method choice after the new pipeline components are done and a preview of the slices (former by applying affine to labelmap and erasing background label, as to create a volume that can be directly displayed by gradio i.e. .obj file and latter via a simple slice selection and image 2d image preview). Generating a volume to allow for easy identification of slice location still wip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yg9340hIcNbR",
        "outputId": "3e359956-23d9-4696-9535-26d269e3db51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a491f0c4808275fa72.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a491f0c4808275fa72.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No need to delete results\n",
            "SEGMENTATING:\n",
            "\n",
            "If you use this tool please cite: https://pubs.rsna.org/doi/10.1148/ryai.230024\n",
            "\n",
            "TotalSegmentator sends anonymous usage statistics. If you want to disable it check the documentation.\n",
            "Downloading model for Task 291 ...\n",
            "Download finished. Extracting...\n",
            "Downloading model for Task 292 ...\n",
            "Download finished. Extracting...\n",
            "Downloading model for Task 293 ...\n",
            "Download finished. Extracting...\n",
            "Downloading model for Task 294 ...\n",
            "Download finished. Extracting...\n",
            "Downloading model for Task 295 ...\n",
            "Download finished. Extracting...\n",
            "Resampling...\n",
            "  Resampled in 3.19s\n",
            "Predicting part 1 of 5 ...\n",
            "Predicting part 2 of 5 ...\n",
            "Predicting part 3 of 5 ...\n",
            "Predicting part 4 of 5 ...\n",
            "Predicting part 5 of 5 ...\n",
            "  Predicted in 140.65s\n",
            "Resampling...\n",
            "SEG DONE!\n",
            "\n",
            "If you use this tool please cite: https://pubs.rsna.org/doi/10.1148/ryai.230024\n",
            "\n",
            "Downloading model for Task 481 ...\n",
            "Download finished. Extracting...\n",
            "Resampling...\n",
            "  Resampled in 0.47s\n",
            "Predicting...\n",
            "  Predicted in 18.86s\n",
            "Resampling...\n",
            "SEG DONE!\n",
            "\n",
            "If you use this tool please cite: https://pubs.rsna.org/doi/10.1148/ryai.230024\n",
            "\n",
            "Downloading model for Task 299 ...\n",
            "Download finished. Extracting...\n",
            "Resampling...\n",
            "  Resampled in 0.48s\n",
            "Predicting...\n",
            "  Predicted in 19.35s\n",
            "Resampling...\n",
            "SEG DONE!\n",
            "ASSEMBLY STARTED: total\n",
            "ASSEMBLY COMPLETED\n",
            "ASSEMBLY STARTED: tissue_types\n",
            "ASSEMBLY COMPLETED\n",
            "ASSEMBLY STARTED: body\n",
            "ASSEMBLY COMPLETED\n",
            "US SIMULATION STARTED\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "US SIMULATION COMPLETED\n",
            "SAVED TO '/CT2US/labels/img0002_label.nii.gz'\n"
          ]
        }
      ],
      "source": [
        "img_dir = this_folder / \"imgs\"\n",
        "label_dir = this_folder / \"labels\"\n",
        "us_dir = this_folder / \"us\"\n",
        "gen_dir = this_folder / \"gen\"\n",
        "\n",
        "os.makedirs(img_dir, exist_ok=True)\n",
        "os.makedirs(label_dir, exist_ok=True)\n",
        "os.makedirs(us_dir, exist_ok=True)\n",
        "os.makedirs(gen_dir, exist_ok=True)\n",
        "\n",
        "os.environ[\"GRADIO_ALLOWED_PATHS\"]=str(this_folder)\n",
        "\n",
        "axis_sample_size = 20000\n",
        "axis_points_rgba = [255,255,255,255]\n",
        "\n",
        "def add_axis_pcd(points, colors, shape, y) -> tri.PointCloud:\n",
        "    pcd_points = points.copy()\n",
        "    pcd_colors = colors.copy()\n",
        "\n",
        "    axis = torch.zeros(shape, device=device)\n",
        "    axis[:,:,y] = 1\n",
        "    tmp = axis.nonzero()\n",
        "\n",
        "    idx_list = list(range(tmp.shape[0]))\n",
        "    select_idx = np.random.choice(idx_list, size=axis_sample_size)\n",
        "    pcd_points.append(tmp[select_idx])\n",
        "\n",
        "    tmp = torch.zeros((axis_sample_size, 4), dtype=torch.uint8, device=device)\n",
        "    tmp[:,...] = torch.as_tensor(axis_points_rgba)\n",
        "    pcd_colors.append(tmp)\n",
        "\n",
        "    point_pos = torch.concat(pcd_points)\n",
        "    colors = torch.concat(pcd_colors)\n",
        "\n",
        "    point_pos = point_pos.float()\n",
        "\n",
        "    point_displacement = torch.rand(point_pos.shape).to(device)\n",
        "    point_pos += point_displacement\n",
        "\n",
        "    shape = torch.FloatTensor(np.array(shape)).to(device)\n",
        "    point_pos /= shape\n",
        "    point_pos = point_pos - .5\n",
        "\n",
        "    pcd = tri.PointCloud(point_pos.cpu(), colors.cpu()).apply_transform(\n",
        "                    np.dot(\n",
        "                        tri.transformations.rotation_matrix(np.pi, [1, 0, 0]),\n",
        "                        tri.transformations.rotation_matrix(np.pi/2, [0, -1, 0])\n",
        "                    )\n",
        "            )\n",
        "\n",
        "    return pcd\n",
        "\n",
        "\n",
        "with gr.Blocks() as ct_2_us:\n",
        "    with gr.Row():\n",
        "        files = gr.State({})\n",
        "        us_list = gr.State({})\n",
        "        warped_list = gr.State({})\n",
        "        label_list = gr.State({})\n",
        "        pcdb_list = gr.State({})\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            ct_imgs = gr.Files(file_types=['.nii', '.nii.gz'], type='filepath', label=\"Select CT images\", interactive=True, file_count='multiple')\n",
        "            step_size = gr.Slider(label=\"Slicing step interval\", minimum=1, maximum=20, value=2, step=1, interactive=True)\n",
        "\n",
        "            with gr.Row():\n",
        "                btn = gr.Button(\"Generate\")\n",
        "                reset = gr.Button(\"Reset\")\n",
        "\n",
        "            @gr.on([reset.click, ct_2_us.load], inputs=None, outputs=[files, us_list, warped_list, label_list, pcdb_list])\n",
        "            def reset_all():\n",
        "                for f in label_dir.glob('*.nii.gz'):\n",
        "                    os.remove(f)\n",
        "                for f in img_dir.glob('*.nii.gz'):\n",
        "                    os.remove(f)\n",
        "                for f in gen_dir.glob('*.glb'):\n",
        "                    os.remove(f)\n",
        "                for f in glob.glob(f\"{us_dir}/*\"):\n",
        "                    shutil.rmtree(f, ignore_errors=True)\n",
        "                try:\n",
        "                    os.remove(f\"{this_folder}/results.zip\")\n",
        "                except:\n",
        "                    print(\"No need to delete results\")\n",
        "                return {}, {}, {}, {}, {}\n",
        "\n",
        "            # gr.Examples([item for item in reduce(lambda result, x: result + [subset + [x] for subset in result], examples, [[]]) if len(item)>0], ct_imgs)\n",
        "            # gr.Examples(\n",
        "            #                 examples=[[str(path)] for path in sorted(pthlib(this_folder / 'sample').glob('**/*.nii'))]\n",
        "            #                         + [[str(path)] for path in sorted(pthlib(this_folder / 'sample').glob('**/*.nii.gz'))],\n",
        "            #                 inputs=[ct_imgs],\n",
        "            #                 cache_examples=False,\n",
        "            #                 label='Sample CT volumes',\n",
        "            #                 examples_per_page=10\n",
        "            #             )\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    sample_in = gr.Dropdown(\n",
        "                                                choices=[i+1 for i in range(len(glob.glob(str(this_folder / 'sample' / '*.nii.gz'))))],\n",
        "                                                label='Amount of samples to randomly select',\n",
        "                                                info='Used for demo with no input'\n",
        "                                            )\n",
        "                    seg_method = gr.Radio(choices=[\"predictor\", \"new\", \"old\"], value=\"old\", label=\"Segmentation method\", interactive=True)\n",
        "                    us_method = gr.Radio(choices=[\"lotus\"], value=\"lotus\", label=\"US rendering method\", interactive=True)\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "\n",
        "            with gr.Tab(label='Preview'):\n",
        "                note = gr.Markdown(label=\"status\", value=\"Generate US images first through the input tab\")\n",
        "\n",
        "                img_idx = gr.State(0)\n",
        "                slice_idx = gr.State(0)\n",
        "\n",
        "                @gr.render(inputs=[files, us_list, warped_list, label_list, pcdb_list, step_size], triggers=[us_list.change])\n",
        "                def dynamic(fl, us, warped, ll, pcdb, step):\n",
        "                    with gr.Column():\n",
        "                        if len(us) > 0:\n",
        "                            dropdown = gr.Dropdown(choices=[(f[0], n) for n, f in fl.items()], label='Select image to preview', value=0)\n",
        "                            slider = gr.Slider(minimum=0, maximum=len(warped[0]) - 1, step=step, label='Slice selection', value=0)\n",
        "\n",
        "                            iden = lambda x: x\n",
        "\n",
        "                            slider.release(fn=iden, inputs=[slider], outputs=[slice_idx])\n",
        "                            dropdown.select(fn=iden, inputs=[dropdown], outputs=[img_idx])\n",
        "\n",
        "                            # States for results and dropdown selection\n",
        "                            with gr.Column():\n",
        "                                # \"original\"\n",
        "                                with gr.Row():\n",
        "                                    base = gr.Image(\n",
        "                                                label='US slice',\n",
        "                                                value=np.asarray(us[img_idx.value][slice_idx.value], dtype=np.float32),\n",
        "                                                height=300\n",
        "                                            )\n",
        "\n",
        "                                    label_preview = gr.Image(\n",
        "                                                        label='Label slice',\n",
        "                                                        value=ll[0][0],\n",
        "                                                        type='pil',\n",
        "                                                        height=300\n",
        "                                                    )\n",
        "                                # BOTTOM ROW -> annotation and 3ddw\n",
        "                                with gr.Row():\n",
        "                                    comp = gr.AnnotatedImage(\n",
        "                                                value=(np.asarray(us[img_idx.value][slice_idx.value], dtype=np.float32), warped[0][0]),\n",
        "                                                height=300\n",
        "                                            )\n",
        "\n",
        "                                    volume_preview = gr.Model3D(clear_color=(0, 0, 0, 1), label=\"Label map view\", value=str(gen_dir / \"current_pcd.glb\"), height=300)\n",
        "\n",
        "                                def route(x, y):\n",
        "                                    b = np.asarray(us[x][y], dtype=np.float32)\n",
        "                                    w = warped[x][y]\n",
        "                                    l = ll[x][y]\n",
        "                                    add_axis_pcd(pcdb[x][0], pcdb[x][1], pcdb[x][2], y * step).export(str(gen_dir / \"current_pcd.glb\"))\n",
        "\n",
        "                                    p = str(gen_dir / \"current_pcd.glb\")\n",
        "                                    return (b, w), b, l, p, y if y <= len(us[x]) else 0, \\\n",
        "                                        gr.Slider(minimum=0, maximum=len(warped[x]) - 1, step=step, label='Slice selection', value=y if y <= len(us[x]) else 0)\n",
        "\n",
        "\n",
        "                                gr.on(triggers=[img_idx.change, slice_idx.change],\n",
        "                                        fn=route,\n",
        "                                        inputs=[img_idx, slice_idx],\n",
        "                                        outputs=[comp, base, label_preview, volume_preview, slice_idx, slider]\n",
        "                                        # outputs=[comp, base, label_preview]\n",
        "                                    )\n",
        "\n",
        "            with gr.Tab(label='Download'):\n",
        "                download = gr.DownloadButton(label=\"\", visible=False)\n",
        "                # TODO：allow for picking of specific results, potentially also generate point clouds into files and allow user to download them?\n",
        "                @gr.render(inputs=[files, us_list, warped_list, label_list, pcdb_list, step_size], triggers=[us_list.change])\n",
        "                def dynamic(fl, us, warped, ll, pcdb, step):\n",
        "                    descr = gr.Markdown(label=\"This can be used to adjust contents of results.zip\")\n",
        "                    configs = gr.CheckboxGroup(choices=[\"Save labels\"], value=[\"Save labels\"], label=\"Options\", interactive=True)\n",
        "                    filename_in = gr.Textbox(label=\"Filename for result zip\", value=\"results\")\n",
        "\n",
        "                    r = []\n",
        "                    r.append(label_dir.glob('*.nii.gz'))\n",
        "                    r.append(us_dir.glob('**/*.png'))\n",
        "\n",
        "\n",
        "                    rezip = gr.Button(\"Reassemble results.zip\")\n",
        "\n",
        "                    @gr.on(rezip.click, inputs=[configs, filename_in], outputs=[download, descr])\n",
        "                    def rezip_files(save_configs, name, r=r):\n",
        "                        with ZipFile(f\"{this_folder}/results.zip\", 'w') as zipObj:\n",
        "                            for f in r[0]:\n",
        "                                zipObj.write(f, os.path.relpath(f, str(this_folder)))\n",
        "                                os.remove(f)\n",
        "                            for f in r[1]:\n",
        "                                zipObj.write(f, os.path.relpath(f, str(this_folder)))\n",
        "\n",
        "                            for f in glob.glob(f\"{us_dir}/*\"):\n",
        "                                shutil.rmtree(f, ignore_errors=True)\n",
        "\n",
        "                        return f\"{this_folder}/{name}.zip\", \"Results have been rezipped\"\n",
        "\n",
        "                def start(ct, step, method, method_us, fl_s, us_s, warped_s, ll_s, pcdb_s, nr_samples, progress=gr.Progress(track_tqdm=True)):\n",
        "                    if ct == None:\n",
        "                        ct = glob.glob(str(this_folder / 'sample' / '*.nii.gz'))\n",
        "                        ct = [this_folder / 'sample' / f for f in ct]\n",
        "\n",
        "                    ct = random.sample(ct, k=nr_samples)\n",
        "\n",
        "                    for f in ct:\n",
        "                        shutil.copyfile(f, img_dir / f.name)\n",
        "                        shutil.rmtree(f, ignore_errors=True)\n",
        "\n",
        "                    local_dataset = CTDataset(\n",
        "                        img_dir=img_dir,\n",
        "                        method=method,\n",
        "                        resample=None\n",
        "                    )\n",
        "                    batch_size = 1\n",
        "\n",
        "                    ct_dataloader = DataLoader(local_dataset, batch_size=batch_size, collate_fn=local_dataset.collate_fn)\n",
        "\n",
        "                    ct2us = CT2US(method=method)\n",
        "\n",
        "                    fl = []\n",
        "                    us = []\n",
        "                    warped = []\n",
        "                    ll = []\n",
        "                    pcdb = []\n",
        "\n",
        "                    for data in progress.tqdm(ct_dataloader, desc=\"Processing batches\"):\n",
        "                        imgs, properties, dest_labels, dest_us = data\n",
        "\n",
        "                        # TODO: Last parameter saves labels. This is intentended to allow for more download options and storing of other intermediary results that might be of interest\n",
        "                        n, u, w, l, b = ct2us(imgs, properties, dest_labels, dest_us, step, True)\n",
        "\n",
        "                        fl.append(n)\n",
        "                        us.append(*u)\n",
        "                        warped.append(w)\n",
        "                        ll.append(*l)\n",
        "                        pcdb.append(*b)\n",
        "\n",
        "\n",
        "                    add_axis_pcd(pcdb[0][0], pcdb[0][1], pcdb[0][2], 0).export(str(gen_dir / \"current_pcd.glb\"))\n",
        "\n",
        "                    fl_s.update(enumerate(fl))\n",
        "                    us_s.update(enumerate(us))\n",
        "                    warped_s.update(enumerate(warped))\n",
        "                    ll_s.update(enumerate(ll))\n",
        "                    pcdb_s.update(enumerate(pcdb))\n",
        "\n",
        "                    return gr.DownloadButton(label=\"Download results as zip\", visible=True, value=f\"{this_folder}/results.zip\"), \\\n",
        "                            fl_s, \\\n",
        "                            us_s, \\\n",
        "                            warped_s, \\\n",
        "                            ll_s, \\\n",
        "                            pcdb_s, \\\n",
        "                            gr.Markdown(value=\"Status\", height=30)\n",
        "\n",
        "                btn.click(\n",
        "                            fn=lambda x: gr.Markdown(label=\"Status\", value=\"\", height=80),\n",
        "                            inputs=btn,\n",
        "                            outputs=note\n",
        "                        ).success(\n",
        "                            fn=start,\n",
        "                            inputs=[ct_imgs, step_size, seg_method, us_method, files, us_list, warped_list, label_list, pcdb_list, sample_in],\n",
        "                            outputs=[download, files, us_list, warped_list, label_list, pcdb_list, note]\n",
        "                        ).success(\n",
        "                            fn=lambda x: gr.Markdown(label=\"\", value=\"\", height=0, visible=False),\n",
        "                            inputs=btn,\n",
        "                            outputs=note\n",
        "                        )\n",
        "\n",
        "            # def run(progress=gr.Progress(track_tqdm=True)):\n",
        "\n",
        "            # btn.click(fn=run,\n",
        "            #             inputs=None,\n",
        "            #             outputs=[files, us_imgs, warped_labels, labels, note])\n",
        "\n",
        "ct_2_us.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "ct2us",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}